{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Extra\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers import LeakyReLU, Concatenate, Dropout\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from JLib.models.utils.instance_normalization import InstanceNormalization\n",
    "from JLib.models.utils.sn import ConvSN2D\n",
    "from JLib.models.utils.calc_output_and_feature_size import calc_output_and_feature_size\n",
    "from JLib.models.utils.attention import Attention\n",
    "from keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "# Custom Libs\n",
    "from JLib.models.utils.calc_output_and_feature_size import calc_output_and_feature_size\n",
    "from JLib.data_utils import save_sample_images, write_log, generate_training_images\n",
    "from JLib.data_utils import generator, generate_label_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Keras Modules\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Lambda, UpSampling2D, Input, concatenate\n",
    "from keras.utils.data_utils import  GeneratorEnqueuer\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JLib.models.discriminator_full import DiscriminatorFull\n",
    "from JLib.models.discriminator_low import DiscriminatorLow\n",
    "from JLib.models.discriminator_medium import DiscriminatorMedium\n",
    "from JLib.models.core_generator import CoreGenerator\n",
    "\n",
    "# Other Modules \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 128\n",
    "width = 128\n",
    "channels = 1\n",
    "epochs = 10\n",
    "gpus = 1\n",
    "batch_size = 5\n",
    "cpus = 2\n",
    "use_multiprocessing = True\n",
    "save_weights_every_n_epochs = 0.01\n",
    "max_queue_size=batch_size * 1\n",
    "img_dir = \"data/Train/\"\n",
    "test_dir = \"data/Test/\"\n",
    "resource_dir = \"data/resources/\"\n",
    "dataset_len = len(os.listdir(img_dir))\n",
    "testset_len = len(os.listdir(test_dir))\n",
    "learning_rate = 0.0002\n",
    "experiment_name = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "decay_rate = 0\n",
    "decay_rate = learning_rate / ((dataset_len / batch_size) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Load filenames\n",
    "#-----------------------------------\n",
    "\n",
    "X = []\n",
    "for filename in os.listdir(img_dir):\n",
    "    X.append(filename)\n",
    "\n",
    "Test = []\n",
    "for filename in os.listdir(test_dir):\n",
    "    Test.append(filename)    \n",
    "    \n",
    "# ----------------------------------\n",
    "#  Create directory for sample data\n",
    "# ----------------------------------\n",
    "\n",
    "main_dir = './output/256/' + experiment_name\n",
    "save_sample_images_dir = main_dir + '/sample_images/'\n",
    "save_validation_images_dir = main_dir + '/validation_images/'\n",
    "weights_dir = main_dir +'/weights/'\n",
    "log_path = main_dir + '/logs/'\n",
    "model_path = main_dir + '/models/'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n",
    "    os.makedirs(save_sample_images_dir)\n",
    "    os.makedirs(save_validation_images_dir)\n",
    "    os.makedirs(log_path)\n",
    "    os.makedirs(weights_dir)\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# ---------------\n",
    "#  Import Models \n",
    "# ---------------\n",
    "    \n",
    "core_generator = CoreGenerator(gpus=gpus, width=width, height=height)\n",
    "discriminator_full = DiscriminatorFull(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "discriminator_medium = DiscriminatorMedium(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "discriminator_low = DiscriminatorLow(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "\n",
    "if os.path.isdir(\"./resources/\"):\n",
    "    core_generator.model.load_weights('./resources/core_generator.h5')\n",
    "    discriminator_full.model.load_weights('./resources/discriminator_full.h5')\n",
    "    discriminator_medium.model.load_weights('./resources/discriminator_medium.h5')\n",
    "    discriminator_low.model.load_weights('./resources/discriminator_low.h5')\n",
    "\n",
    "# Create a directory to save weights\n",
    "if not os.path.exists(resource_dir):\n",
    "    os.makedirs(resource_dir)\n",
    "\n",
    "discriminator_full.trainable = False\n",
    "discriminator_medium.model.trainable = False\n",
    "discriminator_full.model.trainable = False\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "#  Create GAN with core generator\n",
    "# --------------------------------\n",
    "\n",
    "# Generate image with core generator\n",
    "gan_x = Input(shape=(height, width, channels,))\n",
    "gan_y = Input(shape=(height, width, 2,))\n",
    "\n",
    "# Extract style features and add them to image\n",
    "gan_output = core_generator.model(gan_x)\n",
    "\n",
    "# Extract features and predictions from discriminators\n",
    "disc_input = concatenate([gan_x, gan_output], axis=-1)\n",
    "pred_full, features_full = discriminator_full.model(disc_input)\n",
    "pred_medium, features_medium = discriminator_medium.model(disc_input)\n",
    "pred_low, features_low = discriminator_low.model(disc_input)\n",
    "\n",
    "# Compile GAN\n",
    "gan_core = Model(inputs=gan_x, outputs=[gan_output, features_full, features_medium, features_low, pred_full, pred_medium, pred_low])                  \n",
    "\n",
    "gan_core.name = \"gan_core\"\n",
    "optimizer = Adam(learning_rate, 0.5, decay=decay_rate)\n",
    "loss_gan = ['mae', 'mae', 'mae', 'mae', 'mse', 'mse', 'mse']\n",
    "loss_weights_gan = [1, 3.33, 3.33, 3.33, 0.33, 0.33, 0.33]\n",
    "\n",
    "# gan_core = multi_gpu_model(gan_core_org)\n",
    "gan_core.compile(optimizer=optimizer, loss_weights=loss_weights_gan, loss=loss_gan)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "#  Compile Discriminator\n",
    "# --------------------------------\n",
    "\n",
    "discriminator_full.model.trainable = True\n",
    "discriminator_medium.model.trainable = True\n",
    "discriminator_low.model.trainable = True\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_true)\n",
    "\n",
    "loss_d = ['mse', zero_loss]\n",
    "loss_weights_d = [1, 0]\n",
    "optimizer_dis = Adam(learning_rate, 0.5, decay=decay_rate)\n",
    "\n",
    "discriminator_full_multi = discriminator_full.model\n",
    "discriminator_medium_multi = discriminator_medium.model\n",
    "discriminator_low_multi = discriminator_low.model\n",
    "\n",
    "discriminator_full_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "discriminator_medium_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "discriminator_low_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "#  Initiate Generator Queue\n",
    "# --------------------------------------------------\n",
    "\n",
    "enqueuer = GeneratorEnqueuer(generator(X, img_dir, batch_size, dataset_len, width, height), use_multiprocessing=use_multiprocessing, wait_time=0.01)\n",
    "\n",
    "enqueuer.start(workers=cpus, max_queue_size=max_queue_size)\n",
    "output_generator = enqueuer.get()\n",
    "\n",
    "# ---------------------------------\n",
    "#  Initiate values for Tensorboard\n",
    "# ---------------------------------\n",
    "\n",
    "callback_Full = TensorBoard(log_path)\n",
    "callback_Medium = TensorBoard(log_path)\n",
    "callback_Low = TensorBoard(log_path)\n",
    "callback_gan = TensorBoard(log_path)\n",
    "\n",
    "callback_Full.set_model(discriminator_full.model)\n",
    "callback_Medium.set_model(discriminator_medium.model)\n",
    "callback_Low.set_model(discriminator_low.model)\n",
    "callback_gan.set_model(gan_core)\n",
    "\n",
    "callback_Full_names = ['weighted_loss_real_full', 'disc_loss_real_full', 'zero_1', 'weighted_loss_fake_full', 'disc_loss_fake_full', 'zero_2']\n",
    "callback_Medium_names = ['weighted_loss_real_low', 'disc_loss_real_medium', 'zero_3', 'weighted_loss_fake_medium', 'disc_loss_fake_medium', 'zero_4']\n",
    "callback_Low_names = ['weighted_loss_real_low', 'disc_loss_real_low', 'zero_3', 'weighted_loss_fake_low', 'disc_loss_fake_low', 'zero_4']\n",
    "callback_gan_names = ['total_gan_loss', 'image_diff', 'feature_diff_disc_full', 'feature_diff_disc_low', 'predictions_full', 'predictions_low']\n",
    "\n",
    "# Decide how often to create sample images, save log data, and weights. \n",
    "cycles = int(epochs * (dataset_len / batch_size))\n",
    "save_images_cycle = int((dataset_len / batch_size))\n",
    "save_weights_cycle = int((dataset_len / batch_size))\n",
    "\n",
    "# Calculate the discriminator output size for features and image predictions\n",
    "pred_size_f, feat_size_f = calc_output_and_feature_size(width, height)\n",
    "pred_size_m, feat_size_m = calc_output_and_feature_size(width/2, height/2)\n",
    "pred_size_l, feat_size_l = calc_output_and_feature_size(width/4, height/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateNumba(x, y):\n",
    "    return np.concatenate([x, y], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, cycles):\n",
    "    start_c = time.time()\n",
    "    # ------------------------\n",
    "    #  Generate Training Data\n",
    "    # ------------------------\n",
    "\n",
    "    # Discriminator data\n",
    "    x_full, y_full, x_and_y_full = next(output_generator)\n",
    "    x_medium, y_medium, x_and_y_medium = next(output_generator)\n",
    "    x_low, y_low, x_and_y_low = next(output_generator)\n",
    "    \n",
    "    # Fixed data\n",
    "    fake_labels_f, true_labels_f, dummy_f = generate_label_data(batch_size, pred_size_f, feat_size_f)\n",
    "    fake_labels_m, true_labels_m, dummy_m = generate_label_data(batch_size, pred_size_m, feat_size_m)\n",
    "    fake_labels_l, true_labels_l, dummy_l = generate_label_data(batch_size, pred_size_l, feat_size_l)\n",
    "  \n",
    "    # GAN data\n",
    "    x_gan, y_gan, x_and_y_gan = next(output_generator)\n",
    "\n",
    "    # ----------------------\n",
    "    #  Train Discriminators \n",
    "    # ----------------------\n",
    "\n",
    "    y_gen_full, _, _, _, _, _, _ = gan_core.predict(x_full)\n",
    "    x_and_y_gen_full = concatenateNumba(x_full, y_gen_full)\n",
    "    \n",
    "    # Prepare data for Medium Resolution Discriminator \n",
    "    y_gen_medium, _, _, _, _ , _, _= gan_core.predict(x_medium)\n",
    "    x_and_y_gen_medium = concatenateNumba(x_medium, y_gen_medium)\n",
    "    \n",
    "    # Prepare data for Low Resolution Discriminator \n",
    "    y_gen_low, _, _, _, _ , _, _= gan_core.predict(x_low)\n",
    "    x_and_y_gen_low = concatenateNumba(x_low, y_gen_low)\n",
    "\n",
    "    # Train Discriminators \n",
    "    d_loss_fake_full = discriminator_full_multi.train_on_batch(x_and_y_gen_full, [fake_labels_f, dummy_f])\n",
    "    d_loss_real_full = discriminator_full_multi.train_on_batch(x_and_y_full, [true_labels_f, dummy_f])\n",
    "    \n",
    "    d_loss_fake_medium = discriminator_medium_multi.train_on_batch(x_and_y_gen_medium, [fake_labels_m, dummy_m])\n",
    "    d_loss_real_medium = discriminator_medium_multi.train_on_batch(x_and_y_medium, [true_labels_m, dummy_m])\n",
    "   \n",
    "    d_loss_fake_low = discriminator_low_multi.train_on_batch(x_and_y_gen_low, [fake_labels_l, dummy_l])\n",
    "    d_loss_real_low = discriminator_low_multi.train_on_batch(x_and_y_low, [true_labels_l, dummy_l])\n",
    "\n",
    "    # -----------\n",
    "    #  Train GAN\n",
    "    # -----------\n",
    "    \n",
    "\n",
    "    # Extract featuers from discriminators \n",
    "    _, real_features_full = discriminator_full_multi.predict(x_and_y_gan)\n",
    "    _, real_features_medium = discriminator_medium_multi.predict(x_and_y_gan)\n",
    "    _, real_features_low = discriminator_low_multi.predict(x_and_y_gan)\n",
    "    \n",
    "    # Train GAN on one batch\n",
    "    gan_core_loss = gan_core.train_on_batch(x_gan, [y_gan, \n",
    "                                                    real_features_full,\n",
    "                                                    real_features_medium,\n",
    "                                                    real_features_low,\n",
    "                                                    true_labels_f,\n",
    "                                                    true_labels_m,\n",
    "                                                    true_labels_l])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, x_y_val = generate_training_images(Test, 5, testset_len, width, height, test_dir)\n",
    "output_benchmark, _, _, _, _, _ ,_ = gan_core.predict(x_val)\n",
    "\n",
    "plt.figure(figsize=(20,10), facecolor='w')\n",
    "for i in range(len(output_benchmark)):\n",
    "    cur = np.zeros((128, 128, 3))\n",
    "    cur[:, :, 0] = x_val[i][:, :, 0] * 100\n",
    "    cur[:, :, 1:] = output_benchmark[i] * 128\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10), facecolor='w')\n",
    "for i in range(len(y_gen_full)):\n",
    "    cur = np.zeros((128, 128, 3))\n",
    "    cur[:, :, 0] = x_full[i][:, :, 0] * 100\n",
    "    cur[:, :, 1:] = y_gen_full[i] * 128\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(lab2rgb(cur))"
   ]
  }
 ]
}