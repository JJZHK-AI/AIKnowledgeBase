{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用ResNet50对MNIST数据集进行分类\n",
    "===\n",
    "![Images](images/04_08_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Images](images/04_08_002.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.全局设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='/input/', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='/input/', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # because MNIST is already 1x1 here:\n",
    "        # disable avg pooling\n",
    "        #x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "def resnet34(num_classes):\n",
    "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
    "    model = ResNet(block=Bottleneck, \n",
    "                   layers=[3, 4, 6, 3],\n",
    "                   num_classes=NUM_CLASSES,\n",
    "                   grayscale=GRAYSCALE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet34(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 0000/0469 | Cost: 2.7476\n",
      "Epoch: 001/020 | Batch 0050/0469 | Cost: 1.7531\n",
      "Epoch: 001/020 | Batch 0100/0469 | Cost: 1.0555\n",
      "Epoch: 001/020 | Batch 0150/0469 | Cost: 0.7628\n",
      "Epoch: 001/020 | Batch 0200/0469 | Cost: 0.6652\n",
      "Epoch: 001/020 | Batch 0250/0469 | Cost: 0.6022\n",
      "Epoch: 001/020 | Batch 0300/0469 | Cost: 0.5114\n",
      "Epoch: 001/020 | Batch 0350/0469 | Cost: 0.4114\n",
      "Epoch: 001/020 | Batch 0400/0469 | Cost: 0.3579\n",
      "Epoch: 001/020 | Batch 0450/0469 | Cost: 0.3847\n",
      "Epoch: 001/020 | Train: 91.287%\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 002/020 | Batch 0000/0469 | Cost: 0.4184\n",
      "Epoch: 002/020 | Batch 0050/0469 | Cost: 0.2322\n",
      "Epoch: 002/020 | Batch 0100/0469 | Cost: 0.2970\n",
      "Epoch: 002/020 | Batch 0150/0469 | Cost: 0.2805\n",
      "Epoch: 002/020 | Batch 0200/0469 | Cost: 0.3483\n",
      "Epoch: 002/020 | Batch 0250/0469 | Cost: 0.2879\n",
      "Epoch: 002/020 | Batch 0300/0469 | Cost: 0.1677\n",
      "Epoch: 002/020 | Batch 0350/0469 | Cost: 0.1563\n",
      "Epoch: 002/020 | Batch 0400/0469 | Cost: 0.3314\n",
      "Epoch: 002/020 | Batch 0450/0469 | Cost: 0.1075\n",
      "Epoch: 002/020 | Train: 96.807%\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 003/020 | Batch 0000/0469 | Cost: 0.1551\n",
      "Epoch: 003/020 | Batch 0050/0469 | Cost: 0.0923\n",
      "Epoch: 003/020 | Batch 0100/0469 | Cost: 0.0926\n",
      "Epoch: 003/020 | Batch 0150/0469 | Cost: 0.1290\n",
      "Epoch: 003/020 | Batch 0200/0469 | Cost: 0.0671\n",
      "Epoch: 003/020 | Batch 0250/0469 | Cost: 0.1849\n",
      "Epoch: 003/020 | Batch 0300/0469 | Cost: 0.0532\n",
      "Epoch: 003/020 | Batch 0350/0469 | Cost: 0.1804\n",
      "Epoch: 003/020 | Batch 0400/0469 | Cost: 0.1723\n",
      "Epoch: 003/020 | Batch 0450/0469 | Cost: 0.2079\n",
      "Epoch: 003/020 | Train: 98.275%\n",
      "Time elapsed: 2.27 min\n",
      "Epoch: 004/020 | Batch 0000/0469 | Cost: 0.0561\n",
      "Epoch: 004/020 | Batch 0050/0469 | Cost: 0.1136\n",
      "Epoch: 004/020 | Batch 0100/0469 | Cost: 0.0879\n",
      "Epoch: 004/020 | Batch 0150/0469 | Cost: 0.0745\n",
      "Epoch: 004/020 | Batch 0200/0469 | Cost: 0.0307\n",
      "Epoch: 004/020 | Batch 0250/0469 | Cost: 0.1248\n",
      "Epoch: 004/020 | Batch 0300/0469 | Cost: 0.0427\n",
      "Epoch: 004/020 | Batch 0350/0469 | Cost: 0.1343\n",
      "Epoch: 004/020 | Batch 0400/0469 | Cost: 0.1268\n",
      "Epoch: 004/020 | Batch 0450/0469 | Cost: 0.0931\n",
      "Epoch: 004/020 | Train: 98.842%\n",
      "Time elapsed: 3.03 min\n",
      "Epoch: 005/020 | Batch 0000/0469 | Cost: 0.0159\n",
      "Epoch: 005/020 | Batch 0050/0469 | Cost: 0.0219\n",
      "Epoch: 005/020 | Batch 0100/0469 | Cost: 0.0488\n",
      "Epoch: 005/020 | Batch 0150/0469 | Cost: 0.0597\n",
      "Epoch: 005/020 | Batch 0200/0469 | Cost: 0.0888\n",
      "Epoch: 005/020 | Batch 0250/0469 | Cost: 0.0562\n",
      "Epoch: 005/020 | Batch 0300/0469 | Cost: 0.0474\n",
      "Epoch: 005/020 | Batch 0350/0469 | Cost: 0.0382\n",
      "Epoch: 005/020 | Batch 0400/0469 | Cost: 0.0731\n",
      "Epoch: 005/020 | Batch 0450/0469 | Cost: 0.0325\n",
      "Epoch: 005/020 | Train: 99.315%\n",
      "Time elapsed: 3.77 min\n",
      "Epoch: 006/020 | Batch 0000/0469 | Cost: 0.0304\n",
      "Epoch: 006/020 | Batch 0050/0469 | Cost: 0.0234\n",
      "Epoch: 006/020 | Batch 0100/0469 | Cost: 0.0132\n",
      "Epoch: 006/020 | Batch 0150/0469 | Cost: 0.0266\n",
      "Epoch: 006/020 | Batch 0200/0469 | Cost: 0.0648\n",
      "Epoch: 006/020 | Batch 0250/0469 | Cost: 0.0196\n",
      "Epoch: 006/020 | Batch 0300/0469 | Cost: 0.0551\n",
      "Epoch: 006/020 | Batch 0350/0469 | Cost: 0.0221\n",
      "Epoch: 006/020 | Batch 0400/0469 | Cost: 0.0761\n",
      "Epoch: 006/020 | Batch 0450/0469 | Cost: 0.0380\n",
      "Epoch: 006/020 | Train: 99.405%\n",
      "Time elapsed: 4.53 min\n",
      "Epoch: 007/020 | Batch 0000/0469 | Cost: 0.0274\n",
      "Epoch: 007/020 | Batch 0050/0469 | Cost: 0.0726\n",
      "Epoch: 007/020 | Batch 0100/0469 | Cost: 0.0039\n",
      "Epoch: 007/020 | Batch 0150/0469 | Cost: 0.0226\n",
      "Epoch: 007/020 | Batch 0200/0469 | Cost: 0.0191\n",
      "Epoch: 007/020 | Batch 0250/0469 | Cost: 0.0133\n",
      "Epoch: 007/020 | Batch 0300/0469 | Cost: 0.0163\n",
      "Epoch: 007/020 | Batch 0350/0469 | Cost: 0.0083\n",
      "Epoch: 007/020 | Batch 0400/0469 | Cost: 0.1418\n",
      "Epoch: 007/020 | Batch 0450/0469 | Cost: 0.0128\n",
      "Epoch: 007/020 | Train: 99.407%\n",
      "Time elapsed: 5.28 min\n",
      "Epoch: 008/020 | Batch 0000/0469 | Cost: 0.0233\n",
      "Epoch: 008/020 | Batch 0050/0469 | Cost: 0.0028\n",
      "Epoch: 008/020 | Batch 0100/0469 | Cost: 0.0075\n",
      "Epoch: 008/020 | Batch 0150/0469 | Cost: 0.0362\n",
      "Epoch: 008/020 | Batch 0200/0469 | Cost: 0.0304\n",
      "Epoch: 008/020 | Batch 0250/0469 | Cost: 0.0400\n",
      "Epoch: 008/020 | Batch 0300/0469 | Cost: 0.0598\n",
      "Epoch: 008/020 | Batch 0350/0469 | Cost: 0.0153\n",
      "Epoch: 008/020 | Batch 0400/0469 | Cost: 0.0264\n",
      "Epoch: 008/020 | Batch 0450/0469 | Cost: 0.0257\n",
      "Epoch: 008/020 | Train: 99.488%\n",
      "Time elapsed: 6.03 min\n",
      "Epoch: 009/020 | Batch 0000/0469 | Cost: 0.0730\n",
      "Epoch: 009/020 | Batch 0050/0469 | Cost: 0.0091\n",
      "Epoch: 009/020 | Batch 0100/0469 | Cost: 0.0229\n",
      "Epoch: 009/020 | Batch 0150/0469 | Cost: 0.0070\n",
      "Epoch: 009/020 | Batch 0200/0469 | Cost: 0.0337\n",
      "Epoch: 009/020 | Batch 0250/0469 | Cost: 0.0218\n",
      "Epoch: 009/020 | Batch 0300/0469 | Cost: 0.0525\n",
      "Epoch: 009/020 | Batch 0350/0469 | Cost: 0.0198\n",
      "Epoch: 009/020 | Batch 0400/0469 | Cost: 0.0318\n",
      "Epoch: 009/020 | Batch 0450/0469 | Cost: 0.0909\n",
      "Epoch: 009/020 | Train: 99.553%\n",
      "Time elapsed: 6.75 min\n",
      "Epoch: 010/020 | Batch 0000/0469 | Cost: 0.0007\n",
      "Epoch: 010/020 | Batch 0050/0469 | Cost: 0.0313\n",
      "Epoch: 010/020 | Batch 0100/0469 | Cost: 0.0292\n",
      "Epoch: 010/020 | Batch 0150/0469 | Cost: 0.0063\n",
      "Epoch: 010/020 | Batch 0200/0469 | Cost: 0.0322\n",
      "Epoch: 010/020 | Batch 0250/0469 | Cost: 0.0474\n",
      "Epoch: 010/020 | Batch 0300/0469 | Cost: 0.0754\n",
      "Epoch: 010/020 | Batch 0350/0469 | Cost: 0.0147\n",
      "Epoch: 010/020 | Batch 0400/0469 | Cost: 0.0030\n",
      "Epoch: 010/020 | Batch 0450/0469 | Cost: 0.0718\n",
      "Epoch: 010/020 | Train: 99.430%\n",
      "Time elapsed: 7.50 min\n",
      "Epoch: 011/020 | Batch 0000/0469 | Cost: 0.0032\n",
      "Epoch: 011/020 | Batch 0050/0469 | Cost: 0.0019\n",
      "Epoch: 011/020 | Batch 0100/0469 | Cost: 0.0017\n",
      "Epoch: 011/020 | Batch 0150/0469 | Cost: 0.0110\n",
      "Epoch: 011/020 | Batch 0200/0469 | Cost: 0.0124\n",
      "Epoch: 011/020 | Batch 0250/0469 | Cost: 0.0043\n",
      "Epoch: 011/020 | Batch 0300/0469 | Cost: 0.0231\n",
      "Epoch: 011/020 | Batch 0350/0469 | Cost: 0.0584\n",
      "Epoch: 011/020 | Batch 0400/0469 | Cost: 0.0224\n",
      "Epoch: 011/020 | Batch 0450/0469 | Cost: 0.0269\n",
      "Epoch: 011/020 | Train: 99.645%\n",
      "Time elapsed: 8.25 min\n",
      "Epoch: 012/020 | Batch 0000/0469 | Cost: 0.0225\n",
      "Epoch: 012/020 | Batch 0050/0469 | Cost: 0.0548\n",
      "Epoch: 012/020 | Batch 0100/0469 | Cost: 0.0673\n",
      "Epoch: 012/020 | Batch 0150/0469 | Cost: 0.0009\n",
      "Epoch: 012/020 | Batch 0200/0469 | Cost: 0.0040\n",
      "Epoch: 012/020 | Batch 0250/0469 | Cost: 0.0302\n",
      "Epoch: 012/020 | Batch 0300/0469 | Cost: 0.0126\n",
      "Epoch: 012/020 | Batch 0350/0469 | Cost: 0.0327\n",
      "Epoch: 012/020 | Batch 0400/0469 | Cost: 0.0014\n",
      "Epoch: 012/020 | Batch 0450/0469 | Cost: 0.0194\n",
      "Epoch: 012/020 | Train: 99.545%\n",
      "Time elapsed: 9.01 min\n",
      "Epoch: 013/020 | Batch 0000/0469 | Cost: 0.0081\n",
      "Epoch: 013/020 | Batch 0050/0469 | Cost: 0.0113\n",
      "Epoch: 013/020 | Batch 0100/0469 | Cost: 0.0340\n",
      "Epoch: 013/020 | Batch 0150/0469 | Cost: 0.0075\n",
      "Epoch: 013/020 | Batch 0200/0469 | Cost: 0.0113\n",
      "Epoch: 013/020 | Batch 0250/0469 | Cost: 0.0280\n",
      "Epoch: 013/020 | Batch 0300/0469 | Cost: 0.0086\n",
      "Epoch: 013/020 | Batch 0350/0469 | Cost: 0.0948\n",
      "Epoch: 013/020 | Batch 0400/0469 | Cost: 0.0009\n",
      "Epoch: 013/020 | Batch 0450/0469 | Cost: 0.0471\n",
      "Epoch: 013/020 | Train: 99.538%\n",
      "Time elapsed: 9.74 min\n",
      "Epoch: 014/020 | Batch 0000/0469 | Cost: 0.0009\n",
      "Epoch: 014/020 | Batch 0050/0469 | Cost: 0.0344\n",
      "Epoch: 014/020 | Batch 0100/0469 | Cost: 0.0408\n",
      "Epoch: 014/020 | Batch 0150/0469 | Cost: 0.0030\n",
      "Epoch: 014/020 | Batch 0200/0469 | Cost: 0.0055\n",
      "Epoch: 014/020 | Batch 0250/0469 | Cost: 0.0521\n",
      "Epoch: 014/020 | Batch 0300/0469 | Cost: 0.0075\n",
      "Epoch: 014/020 | Batch 0350/0469 | Cost: 0.0207\n",
      "Epoch: 014/020 | Batch 0400/0469 | Cost: 0.0231\n",
      "Epoch: 014/020 | Batch 0450/0469 | Cost: 0.0530\n",
      "Epoch: 014/020 | Train: 99.572%\n",
      "Time elapsed: 10.50 min\n",
      "Epoch: 015/020 | Batch 0000/0469 | Cost: 0.0221\n",
      "Epoch: 015/020 | Batch 0050/0469 | Cost: 0.0064\n",
      "Epoch: 015/020 | Batch 0100/0469 | Cost: 0.0041\n",
      "Epoch: 015/020 | Batch 0150/0469 | Cost: 0.0024\n",
      "Epoch: 015/020 | Batch 0200/0469 | Cost: 0.0028\n",
      "Epoch: 015/020 | Batch 0250/0469 | Cost: 0.0196\n",
      "Epoch: 015/020 | Batch 0300/0469 | Cost: 0.0026\n",
      "Epoch: 015/020 | Batch 0350/0469 | Cost: 0.0131\n",
      "Epoch: 015/020 | Batch 0400/0469 | Cost: 0.0350\n",
      "Epoch: 015/020 | Batch 0450/0469 | Cost: 0.0757\n",
      "Epoch: 015/020 | Train: 99.682%\n",
      "Time elapsed: 11.24 min\n",
      "Epoch: 016/020 | Batch 0000/0469 | Cost: 0.0020\n",
      "Epoch: 016/020 | Batch 0050/0469 | Cost: 0.0225\n",
      "Epoch: 016/020 | Batch 0100/0469 | Cost: 0.0019\n",
      "Epoch: 016/020 | Batch 0150/0469 | Cost: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016/020 | Batch 0200/0469 | Cost: 0.0031\n",
      "Epoch: 016/020 | Batch 0250/0469 | Cost: 0.0869\n",
      "Epoch: 016/020 | Batch 0300/0469 | Cost: 0.0176\n",
      "Epoch: 016/020 | Batch 0350/0469 | Cost: 0.0050\n",
      "Epoch: 016/020 | Batch 0400/0469 | Cost: 0.0025\n",
      "Epoch: 016/020 | Batch 0450/0469 | Cost: 0.0082\n",
      "Epoch: 016/020 | Train: 99.765%\n",
      "Time elapsed: 11.96 min\n",
      "Epoch: 017/020 | Batch 0000/0469 | Cost: 0.0128\n",
      "Epoch: 017/020 | Batch 0050/0469 | Cost: 0.0690\n",
      "Epoch: 017/020 | Batch 0100/0469 | Cost: 0.0315\n",
      "Epoch: 017/020 | Batch 0150/0469 | Cost: 0.0156\n",
      "Epoch: 017/020 | Batch 0200/0469 | Cost: 0.0046\n",
      "Epoch: 017/020 | Batch 0250/0469 | Cost: 0.0340\n",
      "Epoch: 017/020 | Batch 0300/0469 | Cost: 0.0026\n",
      "Epoch: 017/020 | Batch 0350/0469 | Cost: 0.0183\n",
      "Epoch: 017/020 | Batch 0400/0469 | Cost: 0.0069\n",
      "Epoch: 017/020 | Batch 0450/0469 | Cost: 0.0061\n",
      "Epoch: 017/020 | Train: 99.700%\n",
      "Time elapsed: 12.68 min\n",
      "Epoch: 018/020 | Batch 0000/0469 | Cost: 0.0050\n",
      "Epoch: 018/020 | Batch 0050/0469 | Cost: 0.0022\n",
      "Epoch: 018/020 | Batch 0100/0469 | Cost: 0.0037\n",
      "Epoch: 018/020 | Batch 0150/0469 | Cost: 0.0574\n",
      "Epoch: 018/020 | Batch 0200/0469 | Cost: 0.0088\n",
      "Epoch: 018/020 | Batch 0250/0469 | Cost: 0.0004\n",
      "Epoch: 018/020 | Batch 0300/0469 | Cost: 0.0006\n",
      "Epoch: 018/020 | Batch 0350/0469 | Cost: 0.0123\n",
      "Epoch: 018/020 | Batch 0400/0469 | Cost: 0.0328\n",
      "Epoch: 018/020 | Batch 0450/0469 | Cost: 0.0025\n",
      "Epoch: 018/020 | Train: 99.680%\n",
      "Time elapsed: 13.42 min\n",
      "Epoch: 019/020 | Batch 0000/0469 | Cost: 0.0038\n",
      "Epoch: 019/020 | Batch 0050/0469 | Cost: 0.0024\n",
      "Epoch: 019/020 | Batch 0100/0469 | Cost: 0.0239\n",
      "Epoch: 019/020 | Batch 0150/0469 | Cost: 0.0252\n",
      "Epoch: 019/020 | Batch 0200/0469 | Cost: 0.0032\n",
      "Epoch: 019/020 | Batch 0250/0469 | Cost: 0.0014\n",
      "Epoch: 019/020 | Batch 0300/0469 | Cost: 0.0070\n",
      "Epoch: 019/020 | Batch 0350/0469 | Cost: 0.0056\n",
      "Epoch: 019/020 | Batch 0400/0469 | Cost: 0.0075\n",
      "Epoch: 019/020 | Batch 0450/0469 | Cost: 0.0026\n",
      "Epoch: 019/020 | Train: 99.595%\n",
      "Time elapsed: 14.17 min\n",
      "Epoch: 020/020 | Batch 0000/0469 | Cost: 0.0524\n",
      "Epoch: 020/020 | Batch 0050/0469 | Cost: 0.0060\n",
      "Epoch: 020/020 | Batch 0100/0469 | Cost: 0.0075\n",
      "Epoch: 020/020 | Batch 0150/0469 | Cost: 0.0187\n",
      "Epoch: 020/020 | Batch 0200/0469 | Cost: 0.0018\n",
      "Epoch: 020/020 | Batch 0250/0469 | Cost: 0.0026\n",
      "Epoch: 020/020 | Batch 0300/0469 | Cost: 0.0009\n",
      "Epoch: 020/020 | Batch 0350/0469 | Cost: 0.0403\n",
      "Epoch: 020/020 | Batch 0400/0469 | Cost: 0.0059\n",
      "Epoch: 020/020 | Batch 0450/0469 | Cost: 0.0501\n",
      "Epoch: 020/020 | Train: 99.787%\n",
      "Time elapsed: 14.88 min\n",
      "Total Training Time: 14.88 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.46%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    features = features\n",
    "    targets = targets\n",
    "    break\n",
    "    \n",
    "nhwc_img = np.transpose(features[0], axes=(1, 2, 0))\n",
    "nhw_img = np.squeeze(nhwc_img.numpy(), axis=2)\n",
    "plt.imshow(nhw_img, cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability 7 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "logits, probas = model(features.to(device)[0, None])\n",
    "print('Probability 7 %.2f%%' % (probas[0][7]*100))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
