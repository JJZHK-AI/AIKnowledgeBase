{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用ResNet101对Cifar10数据集进行分类\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.全局设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = torch.device('cuda:0')\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_indices = torch.arange(0, 49000)\n",
    "valid_indices = torch.arange(49000, 50000)\n",
    "\n",
    "\n",
    "train_and_valid = datasets.CIFAR10(root='/input/', \n",
    "                                   train=True, \n",
    "                                   transform=transforms.ToTensor(),\n",
    "                                   download=True)\n",
    "\n",
    "train_dataset = Subset(train_and_valid, train_indices)\n",
    "valid_dataset = Subset(train_and_valid, valid_indices)\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='/input/', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "        #self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "def resnet101(num_classes, grayscale):\n",
    "    \"\"\"Constructs a ResNet-101 model.\"\"\"\n",
    "    model = ResNet(block=Bottleneck, \n",
    "                   layers=[3, 4, 23, 3],\n",
    "                   num_classes=NUM_CLASSES,\n",
    "                   grayscale=grayscale)\n",
    "    return model\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "##########################\n",
    "### COST AND OPTIMIZER\n",
    "##########################\n",
    "\n",
    "model = resnet101(NUM_CLASSES, GRAYSCALE)\n",
    "model.to(DEVICE)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 000/383 | Cost: 2.5346\n",
      "Epoch: 001/050 | Batch 120/383 | Cost: 4.2170\n",
      "Epoch: 001/050 | Batch 240/383 | Cost: 2.1526\n",
      "Epoch: 001/050 | Batch 360/383 | Cost: 2.0389\n",
      "Epoch: 001/050 Train Acc.: 22.33% | Validation Acc.: 22.60%\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 002/050 | Batch 000/383 | Cost: 1.9905\n",
      "Epoch: 002/050 | Batch 120/383 | Cost: 1.9025\n",
      "Epoch: 002/050 | Batch 240/383 | Cost: 1.8451\n",
      "Epoch: 002/050 | Batch 360/383 | Cost: 1.6518\n",
      "Epoch: 002/050 Train Acc.: 33.55% | Validation Acc.: 35.40%\n",
      "Time elapsed: 1.84 min\n",
      "Epoch: 003/050 | Batch 000/383 | Cost: 1.7696\n",
      "Epoch: 003/050 | Batch 120/383 | Cost: 1.4954\n",
      "Epoch: 003/050 | Batch 240/383 | Cost: 1.5155\n",
      "Epoch: 003/050 | Batch 360/383 | Cost: 1.5937\n",
      "Epoch: 003/050 Train Acc.: 46.76% | Validation Acc.: 48.60%\n",
      "Time elapsed: 2.76 min\n",
      "Epoch: 004/050 | Batch 000/383 | Cost: 1.4683\n",
      "Epoch: 004/050 | Batch 120/383 | Cost: 1.2809\n",
      "Epoch: 004/050 | Batch 240/383 | Cost: 1.5195\n",
      "Epoch: 004/050 | Batch 360/383 | Cost: 1.2811\n",
      "Epoch: 004/050 Train Acc.: 54.29% | Validation Acc.: 52.20%\n",
      "Time elapsed: 3.69 min\n",
      "Epoch: 005/050 | Batch 000/383 | Cost: 1.3078\n",
      "Epoch: 005/050 | Batch 120/383 | Cost: 1.1933\n",
      "Epoch: 005/050 | Batch 240/383 | Cost: 1.0962\n",
      "Epoch: 005/050 | Batch 360/383 | Cost: 1.1990\n",
      "Epoch: 005/050 Train Acc.: 58.86% | Validation Acc.: 58.10%\n",
      "Time elapsed: 4.61 min\n",
      "Epoch: 006/050 | Batch 000/383 | Cost: 1.0551\n",
      "Epoch: 006/050 | Batch 120/383 | Cost: 1.1577\n",
      "Epoch: 006/050 | Batch 240/383 | Cost: 1.1602\n",
      "Epoch: 006/050 | Batch 360/383 | Cost: 1.1413\n",
      "Epoch: 006/050 Train Acc.: 61.49% | Validation Acc.: 59.00%\n",
      "Time elapsed: 5.53 min\n",
      "Epoch: 007/050 | Batch 000/383 | Cost: 1.0811\n",
      "Epoch: 007/050 | Batch 120/383 | Cost: 0.9801\n",
      "Epoch: 007/050 | Batch 240/383 | Cost: 0.8606\n",
      "Epoch: 007/050 | Batch 360/383 | Cost: 1.0857\n",
      "Epoch: 007/050 Train Acc.: 69.11% | Validation Acc.: 66.40%\n",
      "Time elapsed: 6.45 min\n",
      "Epoch: 008/050 | Batch 000/383 | Cost: 0.8295\n",
      "Epoch: 008/050 | Batch 120/383 | Cost: 0.9112\n",
      "Epoch: 008/050 | Batch 240/383 | Cost: 1.0460\n",
      "Epoch: 008/050 | Batch 360/383 | Cost: 0.8048\n",
      "Epoch: 008/050 Train Acc.: 61.40% | Validation Acc.: 55.70%\n",
      "Time elapsed: 7.35 min\n",
      "Epoch: 009/050 | Batch 000/383 | Cost: 1.2434\n",
      "Epoch: 009/050 | Batch 120/383 | Cost: 1.1365\n",
      "Epoch: 009/050 | Batch 240/383 | Cost: 0.9072\n",
      "Epoch: 009/050 | Batch 360/383 | Cost: 0.7647\n",
      "Epoch: 009/050 Train Acc.: 72.60% | Validation Acc.: 66.60%\n",
      "Time elapsed: 8.26 min\n",
      "Epoch: 010/050 | Batch 000/383 | Cost: 0.8125\n",
      "Epoch: 010/050 | Batch 120/383 | Cost: 0.8553\n",
      "Epoch: 010/050 | Batch 240/383 | Cost: 0.8545\n",
      "Epoch: 010/050 | Batch 360/383 | Cost: 1.0821\n",
      "Epoch: 010/050 Train Acc.: 70.33% | Validation Acc.: 65.70%\n",
      "Time elapsed: 9.18 min\n",
      "Epoch: 011/050 | Batch 000/383 | Cost: 0.8476\n",
      "Epoch: 011/050 | Batch 120/383 | Cost: 0.8422\n",
      "Epoch: 011/050 | Batch 240/383 | Cost: 0.7926\n",
      "Epoch: 011/050 | Batch 360/383 | Cost: 0.8506\n",
      "Epoch: 011/050 Train Acc.: 78.45% | Validation Acc.: 72.10%\n",
      "Time elapsed: 10.11 min\n",
      "Epoch: 012/050 | Batch 000/383 | Cost: 0.5404\n",
      "Epoch: 012/050 | Batch 120/383 | Cost: 0.8869\n",
      "Epoch: 012/050 | Batch 240/383 | Cost: 0.6769\n",
      "Epoch: 012/050 | Batch 360/383 | Cost: 0.7718\n",
      "Epoch: 012/050 Train Acc.: 82.76% | Validation Acc.: 74.50%\n",
      "Time elapsed: 11.02 min\n",
      "Epoch: 013/050 | Batch 000/383 | Cost: 0.4642\n",
      "Epoch: 013/050 | Batch 120/383 | Cost: 0.4114\n",
      "Epoch: 013/050 | Batch 240/383 | Cost: 0.4873\n",
      "Epoch: 013/050 | Batch 360/383 | Cost: 0.5417\n",
      "Epoch: 013/050 Train Acc.: 85.10% | Validation Acc.: 73.90%\n",
      "Time elapsed: 11.94 min\n",
      "Epoch: 014/050 | Batch 000/383 | Cost: 0.4915\n",
      "Epoch: 014/050 | Batch 120/383 | Cost: 0.5188\n",
      "Epoch: 014/050 | Batch 240/383 | Cost: 0.5432\n",
      "Epoch: 014/050 | Batch 360/383 | Cost: 0.5713\n",
      "Epoch: 014/050 Train Acc.: 87.11% | Validation Acc.: 73.60%\n",
      "Time elapsed: 12.86 min\n",
      "Epoch: 015/050 | Batch 000/383 | Cost: 0.4651\n",
      "Epoch: 015/050 | Batch 120/383 | Cost: 0.3519\n",
      "Epoch: 015/050 | Batch 240/383 | Cost: 0.4224\n",
      "Epoch: 015/050 | Batch 360/383 | Cost: 0.3921\n",
      "Epoch: 015/050 Train Acc.: 89.97% | Validation Acc.: 75.30%\n",
      "Time elapsed: 13.79 min\n",
      "Epoch: 016/050 | Batch 000/383 | Cost: 0.2310\n",
      "Epoch: 016/050 | Batch 120/383 | Cost: 0.4286\n",
      "Epoch: 016/050 | Batch 240/383 | Cost: 0.4030\n",
      "Epoch: 016/050 | Batch 360/383 | Cost: 0.5194\n",
      "Epoch: 016/050 Train Acc.: 91.23% | Validation Acc.: 76.50%\n",
      "Time elapsed: 14.72 min\n",
      "Epoch: 017/050 | Batch 000/383 | Cost: 0.3005\n",
      "Epoch: 017/050 | Batch 120/383 | Cost: 0.2383\n",
      "Epoch: 017/050 | Batch 240/383 | Cost: 0.3543\n",
      "Epoch: 017/050 | Batch 360/383 | Cost: 0.3656\n",
      "Epoch: 017/050 Train Acc.: 92.28% | Validation Acc.: 76.00%\n",
      "Time elapsed: 15.63 min\n",
      "Epoch: 018/050 | Batch 000/383 | Cost: 0.1780\n",
      "Epoch: 018/050 | Batch 120/383 | Cost: 0.2261\n",
      "Epoch: 018/050 | Batch 240/383 | Cost: 0.2144\n",
      "Epoch: 018/050 | Batch 360/383 | Cost: 0.2318\n",
      "Epoch: 018/050 Train Acc.: 93.16% | Validation Acc.: 73.70%\n",
      "Time elapsed: 16.55 min\n",
      "Epoch: 019/050 | Batch 000/383 | Cost: 0.1288\n",
      "Epoch: 019/050 | Batch 120/383 | Cost: 0.1294\n",
      "Epoch: 019/050 | Batch 240/383 | Cost: 0.2731\n",
      "Epoch: 019/050 | Batch 360/383 | Cost: 0.2751\n",
      "Epoch: 019/050 Train Acc.: 94.36% | Validation Acc.: 73.90%\n",
      "Time elapsed: 17.46 min\n",
      "Epoch: 020/050 | Batch 000/383 | Cost: 0.0955\n",
      "Epoch: 020/050 | Batch 120/383 | Cost: 0.1983\n",
      "Epoch: 020/050 | Batch 240/383 | Cost: 0.2700\n",
      "Epoch: 020/050 | Batch 360/383 | Cost: 0.2092\n",
      "Epoch: 020/050 Train Acc.: 94.85% | Validation Acc.: 74.50%\n",
      "Time elapsed: 18.38 min\n",
      "Epoch: 021/050 | Batch 000/383 | Cost: 0.1341\n",
      "Epoch: 021/050 | Batch 120/383 | Cost: 0.2478\n",
      "Epoch: 021/050 | Batch 240/383 | Cost: 0.2588\n",
      "Epoch: 021/050 | Batch 360/383 | Cost: 0.2110\n",
      "Epoch: 021/050 Train Acc.: 94.34% | Validation Acc.: 75.10%\n",
      "Time elapsed: 19.29 min\n",
      "Epoch: 022/050 | Batch 000/383 | Cost: 0.1720\n",
      "Epoch: 022/050 | Batch 120/383 | Cost: 0.0814\n",
      "Epoch: 022/050 | Batch 240/383 | Cost: 0.3025\n",
      "Epoch: 022/050 | Batch 360/383 | Cost: 0.1354\n",
      "Epoch: 022/050 Train Acc.: 95.66% | Validation Acc.: 74.20%\n",
      "Time elapsed: 20.21 min\n",
      "Epoch: 023/050 | Batch 000/383 | Cost: 0.1780\n",
      "Epoch: 023/050 | Batch 120/383 | Cost: 0.0682\n",
      "Epoch: 023/050 | Batch 240/383 | Cost: 0.1973\n",
      "Epoch: 023/050 | Batch 360/383 | Cost: 0.3605\n",
      "Epoch: 023/050 Train Acc.: 96.61% | Validation Acc.: 73.50%\n",
      "Time elapsed: 21.14 min\n",
      "Epoch: 024/050 | Batch 000/383 | Cost: 0.0830\n",
      "Epoch: 024/050 | Batch 120/383 | Cost: 0.1073\n",
      "Epoch: 024/050 | Batch 240/383 | Cost: 0.1116\n",
      "Epoch: 024/050 | Batch 360/383 | Cost: 0.2371\n",
      "Epoch: 024/050 Train Acc.: 96.09% | Validation Acc.: 75.20%\n",
      "Time elapsed: 22.06 min\n",
      "Epoch: 025/050 | Batch 000/383 | Cost: 0.0992\n",
      "Epoch: 025/050 | Batch 120/383 | Cost: 0.0844\n",
      "Epoch: 025/050 | Batch 240/383 | Cost: 0.2769\n",
      "Epoch: 025/050 | Batch 360/383 | Cost: 0.1060\n",
      "Epoch: 025/050 Train Acc.: 96.54% | Validation Acc.: 74.20%\n",
      "Time elapsed: 22.97 min\n",
      "Epoch: 026/050 | Batch 000/383 | Cost: 0.0628\n",
      "Epoch: 026/050 | Batch 120/383 | Cost: 0.0617\n",
      "Epoch: 026/050 | Batch 240/383 | Cost: 0.1234\n",
      "Epoch: 026/050 | Batch 360/383 | Cost: 0.1892\n",
      "Epoch: 026/050 Train Acc.: 96.98% | Validation Acc.: 76.10%\n",
      "Time elapsed: 23.89 min\n",
      "Epoch: 027/050 | Batch 000/383 | Cost: 0.1076\n",
      "Epoch: 027/050 | Batch 120/383 | Cost: 0.1571\n",
      "Epoch: 027/050 | Batch 240/383 | Cost: 0.2048\n",
      "Epoch: 027/050 | Batch 360/383 | Cost: 0.1079\n",
      "Epoch: 027/050 Train Acc.: 97.32% | Validation Acc.: 74.40%\n",
      "Time elapsed: 24.82 min\n",
      "Epoch: 028/050 | Batch 000/383 | Cost: 0.0919\n",
      "Epoch: 028/050 | Batch 120/383 | Cost: 0.1019\n",
      "Epoch: 028/050 | Batch 240/383 | Cost: 0.1319\n",
      "Epoch: 028/050 | Batch 360/383 | Cost: 0.0762\n",
      "Epoch: 028/050 Train Acc.: 97.10% | Validation Acc.: 75.30%\n",
      "Time elapsed: 25.74 min\n",
      "Epoch: 029/050 | Batch 000/383 | Cost: 0.0742\n",
      "Epoch: 029/050 | Batch 120/383 | Cost: 0.1326\n",
      "Epoch: 029/050 | Batch 240/383 | Cost: 0.1425\n",
      "Epoch: 029/050 | Batch 360/383 | Cost: 0.2244\n",
      "Epoch: 029/050 Train Acc.: 97.24% | Validation Acc.: 74.70%\n",
      "Time elapsed: 26.67 min\n",
      "Epoch: 030/050 | Batch 000/383 | Cost: 0.0737\n",
      "Epoch: 030/050 | Batch 120/383 | Cost: 0.1225\n",
      "Epoch: 030/050 | Batch 240/383 | Cost: 0.0297\n",
      "Epoch: 030/050 | Batch 360/383 | Cost: 0.0945\n",
      "Epoch: 030/050 Train Acc.: 97.80% | Validation Acc.: 75.80%\n",
      "Time elapsed: 27.60 min\n",
      "Epoch: 031/050 | Batch 000/383 | Cost: 0.1061\n",
      "Epoch: 031/050 | Batch 120/383 | Cost: 0.1287\n",
      "Epoch: 031/050 | Batch 240/383 | Cost: 0.1397\n",
      "Epoch: 031/050 | Batch 360/383 | Cost: 0.1475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031/050 Train Acc.: 97.55% | Validation Acc.: 75.90%\n",
      "Time elapsed: 28.52 min\n",
      "Epoch: 032/050 | Batch 000/383 | Cost: 0.1077\n",
      "Epoch: 032/050 | Batch 120/383 | Cost: 0.0713\n",
      "Epoch: 032/050 | Batch 240/383 | Cost: 0.0614\n",
      "Epoch: 032/050 | Batch 360/383 | Cost: 2.7262\n",
      "Epoch: 032/050 Train Acc.: 54.23% | Validation Acc.: 52.20%\n",
      "Time elapsed: 29.44 min\n",
      "Epoch: 033/050 | Batch 000/383 | Cost: 1.5062\n",
      "Epoch: 033/050 | Batch 120/383 | Cost: 0.6645\n",
      "Epoch: 033/050 | Batch 240/383 | Cost: 0.3153\n",
      "Epoch: 033/050 | Batch 360/383 | Cost: 0.4206\n",
      "Epoch: 033/050 Train Acc.: 93.14% | Validation Acc.: 74.00%\n",
      "Time elapsed: 30.37 min\n",
      "Epoch: 034/050 | Batch 000/383 | Cost: 0.1637\n",
      "Epoch: 034/050 | Batch 120/383 | Cost: 0.1569\n",
      "Epoch: 034/050 | Batch 240/383 | Cost: 0.1052\n",
      "Epoch: 034/050 | Batch 360/383 | Cost: 0.2288\n",
      "Epoch: 034/050 Train Acc.: 97.93% | Validation Acc.: 74.90%\n",
      "Time elapsed: 31.29 min\n",
      "Epoch: 035/050 | Batch 000/383 | Cost: 0.0587\n",
      "Epoch: 035/050 | Batch 120/383 | Cost: 0.0240\n",
      "Epoch: 035/050 | Batch 240/383 | Cost: 0.1219\n",
      "Epoch: 035/050 | Batch 360/383 | Cost: 0.0985\n",
      "Epoch: 035/050 Train Acc.: 98.61% | Validation Acc.: 76.20%\n",
      "Time elapsed: 32.21 min\n",
      "Epoch: 036/050 | Batch 000/383 | Cost: 0.0270\n",
      "Epoch: 036/050 | Batch 120/383 | Cost: 0.0350\n",
      "Epoch: 036/050 | Batch 240/383 | Cost: 0.0436\n",
      "Epoch: 036/050 | Batch 360/383 | Cost: 0.1062\n",
      "Epoch: 036/050 Train Acc.: 97.76% | Validation Acc.: 75.10%\n",
      "Time elapsed: 33.09 min\n",
      "Epoch: 037/050 | Batch 000/383 | Cost: 0.0962\n",
      "Epoch: 037/050 | Batch 120/383 | Cost: 0.0264\n",
      "Epoch: 037/050 | Batch 240/383 | Cost: 0.1131\n",
      "Epoch: 037/050 | Batch 360/383 | Cost: 0.1059\n",
      "Epoch: 037/050 Train Acc.: 98.54% | Validation Acc.: 76.00%\n",
      "Time elapsed: 34.03 min\n",
      "Epoch: 038/050 | Batch 000/383 | Cost: 0.0681\n",
      "Epoch: 038/050 | Batch 120/383 | Cost: 0.0262\n",
      "Epoch: 038/050 | Batch 240/383 | Cost: 0.0414\n",
      "Epoch: 038/050 | Batch 360/383 | Cost: 0.0599\n",
      "Epoch: 038/050 Train Acc.: 98.58% | Validation Acc.: 76.90%\n",
      "Time elapsed: 34.94 min\n",
      "Epoch: 039/050 | Batch 000/383 | Cost: 0.0343\n",
      "Epoch: 039/050 | Batch 120/383 | Cost: 0.0825\n",
      "Epoch: 039/050 | Batch 240/383 | Cost: 0.0578\n",
      "Epoch: 039/050 | Batch 360/383 | Cost: 0.1126\n",
      "Epoch: 039/050 Train Acc.: 98.14% | Validation Acc.: 74.20%\n",
      "Time elapsed: 35.85 min\n",
      "Epoch: 040/050 | Batch 000/383 | Cost: 0.0533\n",
      "Epoch: 040/050 | Batch 120/383 | Cost: 0.0620\n",
      "Epoch: 040/050 | Batch 240/383 | Cost: 0.0491\n",
      "Epoch: 040/050 | Batch 360/383 | Cost: 0.0581\n",
      "Epoch: 040/050 Train Acc.: 98.68% | Validation Acc.: 75.10%\n",
      "Time elapsed: 36.77 min\n",
      "Epoch: 041/050 | Batch 000/383 | Cost: 0.1014\n",
      "Epoch: 041/050 | Batch 120/383 | Cost: 0.0085\n",
      "Epoch: 041/050 | Batch 240/383 | Cost: 0.0966\n",
      "Epoch: 041/050 | Batch 360/383 | Cost: 0.0615\n",
      "Epoch: 041/050 Train Acc.: 97.69% | Validation Acc.: 74.60%\n",
      "Time elapsed: 37.70 min\n",
      "Epoch: 042/050 | Batch 000/383 | Cost: 0.0689\n",
      "Epoch: 042/050 | Batch 120/383 | Cost: 0.0579\n",
      "Epoch: 042/050 | Batch 240/383 | Cost: 0.0874\n",
      "Epoch: 042/050 | Batch 360/383 | Cost: 0.0633\n",
      "Epoch: 042/050 Train Acc.: 98.55% | Validation Acc.: 74.60%\n",
      "Time elapsed: 38.61 min\n",
      "Epoch: 043/050 | Batch 000/383 | Cost: 0.0407\n",
      "Epoch: 043/050 | Batch 120/383 | Cost: 0.0049\n",
      "Epoch: 043/050 | Batch 240/383 | Cost: 0.0769\n",
      "Epoch: 043/050 | Batch 360/383 | Cost: 0.0859\n",
      "Epoch: 043/050 Train Acc.: 98.45% | Validation Acc.: 75.30%\n",
      "Time elapsed: 39.55 min\n",
      "Epoch: 044/050 | Batch 000/383 | Cost: 0.0490\n",
      "Epoch: 044/050 | Batch 120/383 | Cost: 0.1706\n",
      "Epoch: 044/050 | Batch 240/383 | Cost: 0.0579\n",
      "Epoch: 044/050 | Batch 360/383 | Cost: 0.0726\n",
      "Epoch: 044/050 Train Acc.: 98.65% | Validation Acc.: 75.80%\n",
      "Time elapsed: 40.47 min\n",
      "Epoch: 045/050 | Batch 000/383 | Cost: 0.0286\n",
      "Epoch: 045/050 | Batch 120/383 | Cost: 0.0961\n",
      "Epoch: 045/050 | Batch 240/383 | Cost: 0.0336\n",
      "Epoch: 045/050 | Batch 360/383 | Cost: 0.1026\n",
      "Epoch: 045/050 Train Acc.: 98.40% | Validation Acc.: 75.70%\n",
      "Time elapsed: 41.40 min\n",
      "Epoch: 046/050 | Batch 000/383 | Cost: 0.0240\n",
      "Epoch: 046/050 | Batch 120/383 | Cost: 0.0363\n",
      "Epoch: 046/050 | Batch 240/383 | Cost: 0.0243\n",
      "Epoch: 046/050 | Batch 360/383 | Cost: 0.0598\n",
      "Epoch: 046/050 Train Acc.: 98.08% | Validation Acc.: 73.80%\n",
      "Time elapsed: 42.32 min\n",
      "Epoch: 047/050 | Batch 000/383 | Cost: 0.1070\n",
      "Epoch: 047/050 | Batch 120/383 | Cost: 0.0455\n",
      "Epoch: 047/050 | Batch 240/383 | Cost: 0.0640\n",
      "Epoch: 047/050 | Batch 360/383 | Cost: 0.0359\n",
      "Epoch: 047/050 Train Acc.: 98.45% | Validation Acc.: 76.00%\n",
      "Time elapsed: 43.24 min\n",
      "Epoch: 048/050 | Batch 000/383 | Cost: 0.0181\n",
      "Epoch: 048/050 | Batch 120/383 | Cost: 0.0641\n",
      "Epoch: 048/050 | Batch 240/383 | Cost: 0.0450\n",
      "Epoch: 048/050 | Batch 360/383 | Cost: 0.0496\n",
      "Epoch: 048/050 Train Acc.: 98.70% | Validation Acc.: 75.80%\n",
      "Time elapsed: 44.18 min\n",
      "Epoch: 049/050 | Batch 000/383 | Cost: 0.0077\n",
      "Epoch: 049/050 | Batch 120/383 | Cost: 0.0500\n",
      "Epoch: 049/050 | Batch 240/383 | Cost: 0.8420\n",
      "Epoch: 049/050 | Batch 360/383 | Cost: 0.0836\n",
      "Epoch: 049/050 Train Acc.: 97.96% | Validation Acc.: 77.20%\n",
      "Time elapsed: 45.10 min\n",
      "Epoch: 050/050 | Batch 000/383 | Cost: 0.0434\n",
      "Epoch: 050/050 | Batch 120/383 | Cost: 0.0561\n",
      "Epoch: 050/050 | Batch 240/383 | Cost: 0.0880\n",
      "Epoch: 050/050 | Batch 360/383 | Cost: 0.0222\n",
      "Epoch: 050/050 Train Acc.: 98.93% | Validation Acc.: 76.70%\n",
      "Time elapsed: 46.01 min\n",
      "Total Training Time: 46.01 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# use random seed for reproducibility (here batch shuffling)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "    \n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 120:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc = compute_accuracy(model, train_loader, device=DEVICE)\n",
    "        valid_acc = compute_accuracy(model, valid_loader, device=DEVICE)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {valid_acc:.2f}%')\n",
    "        \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "  \n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 74.83%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
