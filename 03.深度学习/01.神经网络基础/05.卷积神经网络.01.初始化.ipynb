{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "神经网络初始化\n",
    "===\n",
    "神经网络的初始化我们将它分为两个部分，分别是神经网络权重的初始化，以及输入数据的预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 1.权重初始化\n",
    "## 1.1.权重初始化的意义\n",
    "模型权重的初始化对于网络的训练很重要, 不好的初始化参数会导致梯度传播问题, 降低训练速度; 而好的初始化参数, 能够加速收敛, 并且更可能找到较优解. 如果权重一开始很小，信号到达最后也会很小；如果权重一开始很大，信号到达最后也会很大。**不合适的权重初始化会使得隐藏层的输入的方差过大,从而在经过sigmoid这种非线性层时离中心较远(导数接近0),因此过早地出现梯度消失**.如使用均值0,标准差为1的正态分布初始化在隐藏层的方差仍会很大. 不初始化为0的原因是若初始化为0,所有的神经元节点开始做的都是同样的计算,最终同层的每个神经元得到相同的参数.好的初始化方法通常只是为了增快学习的速度(加速收敛),在某些网络结构中甚至能够提高准确率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.权重对于神经网络的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512)\n",
    "    x = a @ x\n",
    "    if torch.isnan(x.std()):\n",
    "        break;\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，网络在第29层的时候发生了梯度爆炸，很明显网络的权重初始化值过大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512) * 0.01\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，梯度发生了消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3.高斯分布初始化$(0,0.01)$\n",
    "初始化为小的随机数，如均值为0，方差为0.01的高斯分布，即用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W = 0.01 * np.random.randn(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这种初始化方法只适用于小型网络，对于深层次的网络，权重小道指反向传播计算中梯度也小，梯度\"信号\"被削弱。方差随着输入数量的增大而增大,可以通过正则化方差来提高权重收敛速率,初始权重的方式为正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "n = 64 #输出神经元的个数\n",
    "w = np.random.randn(n) / math.sqrt(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这会使得中间结果$z=\\sum_iw_ix_i+b$的方差较小,神经元不会饱和,学习速度不会减慢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4.高斯分布初始化-$\\frac{2}{n}$\n",
    "ReLU激活函数喜欢的初始化形式为$\\frac{2.0}{n}$，高斯分布权重初始化为如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "n = 64 #输出神经元的个数\n",
    "w = np.random.randn(n) / math.sqrt(2.0 / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "另外用形式$\\frac{2}{n_{in} + n_{out}}$也是推荐的。不过对于以上两种方法，仅仅考虑的每层输入的方差，而后两种方式考虑输入与输出的方差，保持每层的输入与输出方差相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.5.Xavier初始化\n",
    "Xavier初始化可以帮助减少梯度弥散问题，使得信号在神经网络中可以传递得更深。是最为常用的神经网络权重初始化方法。算法根据输入和输出神经元的数量自动决定初始化的范围，算法如下：\n",
    "> 定义参数所在的层的输入维度为n,输出维度为m,那么参数将从$[-\\sqrt{\\frac{6}{m+n}, \\sqrt{\\frac{6}{m+n}}}]$均匀分布采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "假设输入一层X，输出一层Y，那么有\n",
    "$$Y=W_1X_1+W_2X_2+...+W_nX_n$$\n",
    "按照独立变量相乘的方差公式，可以计算出：\n",
    "$$Var(W_iX_i)=E[X_i]^2Var(W_i)+E[W_i]^2Var(X_i)+Var(W_i)Var(X_i)$$\n",
    "我们期望输入X和权重W都是零均值，因此简化为\n",
    "$$Var(X_iX_i)=Var(W_i)Var(X_i)$$\n",
    "进一步假设所有的$X_i,W_i$都是独立同分布，则有：\n",
    "$$Var(Y)=Var(W_1X_1+W_2X_2+...+W_nX_n)=nVar(W_i)Var(X_i)$$\n",
    "即输出的方差与输入有关，为使输出的方差与输入相同，意味着使$nVar(W_i)=1$，因此$Var(W_i)=\\frac{1}{n}=\\frac{1}{n_{in}}$。如果对反向传播的梯度运用同样的步骤，可得$Var(W_i)=\\frac{1}{n_{out}}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "由于$n_{in},n_{out}$通常不相等，所以这两个方差无法同时满足，作为一种折中的方案，可使用介于$\\frac{1}{n_{in}},\\frac{1}{n_{out}}$之间的数来代替：简单的选择是\n",
    "$$Var(W_i)=\\frac{2}{n_{in}+n_{out}}$$\n",
    "可以根据均匀分布的方差，反推出W的均匀分布：\n",
    "$$Var=\\frac{(b-a)^2}{12}$$\n",
    "使其零均值，则$b=-a$，有$Var=\\frac{(2b)^2}{12}=\\frac{2}{n_{in}+n_{out}}$，可得$b=\\frac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}}$，因此Xavier初始化的就是按照下面的均匀分布\n",
    "$$W \\sim U[-\\sqrt{\\frac{6}{m+n}, \\sqrt{\\frac{6}{m+n}}}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0011), tensor(0.0778))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def xavier(m,h):\n",
    "    return torch.Tensor(m,h).uniform_(-1, 1) * math.sqrt(6. / (m+h))\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = torch.tanh(a @ x)\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.6.MSRA初始化\n",
    "MSRA方法是对于Xavier的改进。Xavier初始化方式为使每层方差一致，从而不会发生前向传播爆炸和反向传播梯度消失等问题。对于ReLU激活函数，其使一半数据变成0，初始时这一半的梯度为0，而tanh和sigmoid等的输出初始时梯度接近于1.因此使用ReLU的网络的参数方差可能会波动。\n",
    "于是使用$Var(W)=\\frac{2}{n_{in}}$放大一倍方差来保持方差的平稳。于是MSRA就是使用均值为0，方差为$\\frac{4}{n_{in} + n_{out}}$的高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7.kaiming初始化\n",
    "当使用关于0对称且在$[-1,1]$内有输出的激活函数(sigmoid或tanh)时，使用Xavier初始化比较合适。但是如果使用ReLU等激活函数会有什么效果呢？\n",
    "- 使用适合给定图层的权重矩阵创建张量，并使用从标准正态分布中随机选择的数字填充它\n",
    "- 将每个随机算则的数字乘以$\\frac{\\sqrt{2}}{\\sqrt{n}}$，其中n是从前一层输出到指定层的连接数fan-in\n",
    "- 偏差张量初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.0151e-16), tensor(1.0094e-15))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def xavier(m,h):\n",
    "    return torch.Tensor(m,h).uniform_(-1, 1) * math.sqrt(6. / (m+h))\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = a @ x\n",
    "    x = x.clamp_min(0.)\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第100层激活输出几乎完全消失了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2182), tensor(0.3193))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def kaiming(m,h):\n",
    "    return torch.randn(m, h) * math.sqrt(2./m)\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = kaiming(512, 512)\n",
    "    x = a @ x\n",
    "    x = x.clamp_min(0.)\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用kaiming初始化，有更好的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 2.偏置初始化\n",
    "通常将偏置初始化为0.若初始化为0.01等值,可能并不能得到好的提升,反而可能下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "net = torch.nn.Sequential()\n",
    "for m in net.modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "    elif isinstance(m, torch.nn.ConvTranspose2d):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "    elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.输入数据预处理\n",
    "数据过来，需要在这一层进行一些数据预处理的操作，常见的3种数据预处理的方法有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1.去均值(Mean subtraction)\n",
    "把输入数据各个维度都中心化到0(只是针对训练集！！！)，首先计算训练集的均值，然后把每一个图片都减去这个均值(测试集上也是减这个均值，不要去计算测试集上的均值)，主要原因是光照会影响.去均值也叫做均值减法，是晕处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心迁移到原点。在numpy中，该操作可以通过代码$X -= np.mean(X, axis=0)$实现。而对于图像，更常用的是对所有像素都减去一个值，可以用$X -= np.mean(X)$实现，也可以在3个颜色通道上分别操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2.归一化Normalization\n",
    "幅度归一化到一定的范围(CNN操作一般不做这个操作，因为RGB就是在0~255这个范围之内的)\n",
    "1. 方法一：先对数据做零中心化(zero-centered)处理，然后每个维度都除以其标准差，$X /= np.std(X, axis=0)$\n",
    "2. 方法二：对每个维度都做归一化，使得每个维度的最大和最小是1和-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3.PCA/白化 Whitening\n",
    "用PCA降维在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构$U,S,V=np.linalg.svd(cov)$。白化是对数据每个特征轴上的幅度归一化(CNN一般也不用)。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数据范围进行归一化，几何解释是，如果数据服从多变量的高斯分布，经过白化后，数据的分布将会是一个均值为0，且协方差相等的矩阵。$Xwhite = \\frac{Xrot}{np.sqrt(S + 1e-5)}$。<br/>\n",
    "左边是原始数据，中间是做了PCA之后的数据，相当于先零中心化，然后做了旋转(去相关性)，右边是做了白化之后的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.总结\n",
    "- 当前的主流初始化方式Xavier,MSRA主要是为了保持每层的输入与输出方差相等, 而参数的分布采用均匀分布或高斯分布均可.\n",
    "- 在广泛采用Batch Normalization的情况下, 使用普通的小方差的高斯分布即可.\n",
    "- 另外, 在迁移学习的情况下, 优先采用预训练的模型进行参数初始化.\n",
    "\n",
    "![images](Images/05_01_001.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.使用Kaiming初始化方法初始化神经网络权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='/input/', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='/input', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(28-1) - 28 + 3) / 2 = 1\n",
    "        # 28x28x4 => 14x14x4\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(14-1) - 28 + 2) = 0                                       \n",
    "        # 14x14x4 => 14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(14-1) - 14 + 3) / 2 = 1                 \n",
    "        # 14x14x8 => 7x7x8                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(7-1) - 14 + 2) = 0\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(7*7*8, num_classes)\n",
    "        \n",
    "        ###############################################\n",
    "        # Reinitialize weights using He initialization\n",
    "        ###############################################\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.detach())\n",
    "                m.bias.detach().zero_()\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.detach())\n",
    "                m.bias.detach().zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        logits = self.linear_1(out.view(-1, 7*7*8))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.4577\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 1.1068\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.6609\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.5353\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.4480\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.3159\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.4545\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.4276\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.1386\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.1409\n",
      "Epoch: 001/010 training accuracy: 91.97%\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.2196\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.1464\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.2625\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.1918\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.1485\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.1230\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.1591\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.1409\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.1404\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.1211\n",
      "Epoch: 002/010 training accuracy: 95.21%\n",
      "Time elapsed: 0.87 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.1288\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.2471\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.1309\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.1889\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.1053\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.1564\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.1236\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.1390\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.1557\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.1658\n",
      "Epoch: 003/010 training accuracy: 96.45%\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.1827\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.0613\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.1967\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.1073\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.1061\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.0967\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.0593\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.1031\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.1504\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.1623\n",
      "Epoch: 004/010 training accuracy: 96.62%\n",
      "Time elapsed: 1.74 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.0469\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.0351\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.1232\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.0434\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.1051\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.1131\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.2228\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.1273\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.1406\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.0650\n",
      "Epoch: 005/010 training accuracy: 97.21%\n",
      "Time elapsed: 2.17 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.0887\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.1365\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.1087\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0794\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.0818\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.1875\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.1788\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.1109\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.1056\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0735\n",
      "Epoch: 006/010 training accuracy: 97.23%\n",
      "Time elapsed: 2.61 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.1303\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.0939\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0863\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.1707\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.0843\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.0881\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.0569\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.0806\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.0787\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.1238\n",
      "Epoch: 007/010 training accuracy: 97.47%\n",
      "Time elapsed: 3.04 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.0738\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0674\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.1883\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.0759\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.0631\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.1168\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.0312\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.0823\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.1272\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.0486\n",
      "Epoch: 008/010 training accuracy: 97.53%\n",
      "Time elapsed: 3.48 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0539\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.1863\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.0641\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0392\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.0661\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.0873\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.1961\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.0719\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0793\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0220\n",
      "Epoch: 009/010 training accuracy: 97.90%\n",
      "Time elapsed: 3.91 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0991\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.0768\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.1982\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0399\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.0343\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.0536\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.1165\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.1025\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.1556\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.1757\n",
      "Epoch: 010/010 training accuracy: 97.79%\n",
      "Time elapsed: 4.35 min\n",
      "Total Training Time: 4.35 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in data_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "    \n",
    "    model = model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "          epoch+1, num_epochs, \n",
    "          compute_accuracy(model, train_loader)))\n",
    "    \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.67%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
