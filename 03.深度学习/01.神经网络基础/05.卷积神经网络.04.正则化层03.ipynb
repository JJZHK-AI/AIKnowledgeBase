{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "卷积神经网络之正则化层\n",
    "==="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.总结\n",
    "![images](Images/05_04_004.png)\n",
    "\n",
    "总的来说，BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.Batch Normalization的意义\n",
    "Batch Normalization是训练神经网络模型的一种有效方法。该方法的目标是将特征(每层激活后的输出)归一化为均值为0，标准差为1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 首先，可以理解为非零均值是指数据不围绕0值分布，但数据中大多数值大于零或小于零。结合高方差问题，数据变得非常大或非常小。这个问题在训练层数很多的神经网络时很常见。特征没有在稳定区间内分布(由小到大)，这将影响网络的优化过程。众所周知，优化神经网络需要使用导数计算。假设一个简单的层计算公式是$y = (Wx + b)$， $y$对$w$的导数是:$dy = dWx$。因此，$x$的取值直接影响导数的取值(当然，神经网络模型中梯度的概念并不是那么简单，但从理论上讲，$x$会影响导数)。因此，如果$x$带来不稳定的变化，其导数可能太大，也可能太小，导致学习模型不稳定。这也意味着当使用Batch Normalization 时我们可以在训练中使用更高的学习率。\n",
    "- Batch Normalization 可以避免$x$值经过非线性激活函数后趋于饱和的现象。因此，它确保激活值不会过高或过低。这有助于权重的学习，当不使用时有些权重可能永远无法进行学习，而用了之后，基本上都可以学习到。这有助于我们减少对参数初始值的依赖。\n",
    "- Batch Normalization也是一种正则化形式，有助于最小化过拟合。使用Batch Normalization，我们不需要使用太多的dropput，这是有意义的，因为我们不需要担心丢失太多的信息，当我们实际使用的时候，仍然建议结合使用这两种技术。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python36964bitanaconda3virtualenv5d618e8c7b894374a565943176146b80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}