{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "卷积神经网络之正则化层\n",
    "==="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.实战\n",
    "该部分代码基于MNIST手写数据集构造了一个四层的全连接层神经网络。通过改变不同参数来测试BN对于模型性能的影响。同时利用TensorFlow实现底层的batch normalization。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/JJZHK/data/input/MNIST_data/\", one_hot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2.构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetWork():\n",
    "    def __init__(self, initial_weights, activation_fn, use_batch_norm):\n",
    "        \"\"\"\n",
    "        初始化网络对象\n",
    "        :param initial_weights: 权重初始化值，是一个list，list中每一个元素是一个权重矩阵\n",
    "        :param activation_fn: 隐层激活函数\n",
    "        :param user_batch_norm: 是否使用batch normalization\n",
    "        \"\"\"\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.name = \"With Batch Norm\" if use_batch_norm else \"Without Batch Norm\"\n",
    "\n",
    "        self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "        # 存储训练准确率\n",
    "        self.training_accuracies = []\n",
    "\n",
    "        self.build_network(initial_weights, activation_fn)\n",
    "\n",
    "    def build_network(self, initial_weights, activation_fn):\n",
    "        \"\"\"\n",
    "        构建网络图\n",
    "        :param initial_weights: 权重初始化，是一个list\n",
    "        :param activation_fn: 隐层激活函数\n",
    "        \"\"\"\n",
    "        self.input_layer = tf.placeholder(tf.float32, [None, initial_weights[0].shape[0]])\n",
    "        layer_in = self.input_layer\n",
    "\n",
    "        # 前向计算（不计算最后输出层）\n",
    "        for layer_weights in initial_weights[:-1]:\n",
    "            layer_in = self.fully_connected(layer_in, layer_weights, activation_fn)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = self.fully_connected(layer_in, initial_weights[-1])\n",
    "\n",
    "    def fully_connected(self, layer_in, layer_weights, activation_fn=None):\n",
    "        \"\"\"\n",
    "        抽象出的全连接层计算\n",
    "        \"\"\"\n",
    "        # 如果使用BN与激活函数\n",
    "        if self.use_batch_norm and activation_fn:\n",
    "            weights = tf.Variable(layer_weights)\n",
    "            linear_output = tf.matmul(layer_in, weights)\n",
    "\n",
    "            # 调用BN接口\n",
    "            batch_normalized_output = tf.layers.batch_normalization(linear_output, training=self.is_training)\n",
    "\n",
    "            return activation_fn(batch_normalized_output)\n",
    "        # 如果不使用BN或激活函数（即普通隐层）\n",
    "        else:\n",
    "            weights = tf.Variable(layer_weights)\n",
    "            bias = tf.Variable(tf.zeros([layer_weights.shape[-1]]))\n",
    "            linear_output = tf.add(tf.matmul(layer_in, weights), bias)\n",
    "\n",
    "            return activation_fn(linear_output) if activation_fn else linear_output\n",
    "\n",
    "    def train(self, sess, learning_rate, training_batches, batches_per_validate_data, save_model=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param sess: TensorFlow Session\n",
    "        :param learning_rate: 学习率\n",
    "        :param training_batches: 用于训练的batch数\n",
    "        :param batches_per_validate_data: 训练多少个batch对validation数据进行一次验证\n",
    "        :param save_model: 存储模型\n",
    "        \"\"\"\n",
    "\n",
    "        # 定义输出label\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # 定义损失函数\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,\n",
    "                                                                                  logits=self.output_layer))\n",
    "\n",
    "        # 准确率\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        #\n",
    "        if self.use_batch_norm:\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        else:\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        # 显示进度条\n",
    "        for i in tqdm.tqdm(range(training_batches)):\n",
    "            batch_x, batch_y = mnist.train.next_batch(60)\n",
    "            sess.run(train_step, feed_dict={self.input_layer: batch_x,\n",
    "                                            labels: batch_y,\n",
    "                                            self.is_training: True})\n",
    "            if i % batches_per_validate_data == 0:\n",
    "                val_accuracy = sess.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                             labels: mnist.validation.labels,\n",
    "                                                             self.is_training: False})\n",
    "                self.training_accuracies.append(val_accuracy)\n",
    "        print(\"{}: The final accuracy on validation data is {}\".format(self.name, val_accuracy))\n",
    "\n",
    "        # 存储模型\n",
    "        if save_model:\n",
    "            tf.train.Saver().save(sess, save_model)\n",
    "\n",
    "    def test(self, sess, test_training_accuracy=False, restore=None):\n",
    "        # 定义label\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # 准确率\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # 是否加载模型\n",
    "        if restore:\n",
    "            tf.train.Saver().restore(sess, restore)\n",
    "\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={self.input_layer: mnist.test.images,\n",
    "                                                      labels: mnist.test.labels,\n",
    "                                                      self.is_training: False})\n",
    "\n",
    "        print(\"{}: The final accuracy on test data is {}\".format(self.name, test_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3.辅助函数\n",
    "辅助函数主要包括两个：\n",
    "- 绘制训练过程的准确率\n",
    "- 测试有BN与没有BN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_training_accuracies(*args, batches_per_validate_data):\n",
    "    \"\"\"\n",
    "    绘制模型在训练过程中的准确率曲线\n",
    "\n",
    "    :param args: 一个或多个NeuralNetWork对象\n",
    "    :param batches_per_validate_data: 训练多少个batch进行一次数据验证\n",
    "    \"\"\"\n",
    "    plt.figure(facecolor='w')\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for nn in args:\n",
    "        ax.plot(range(0, len(nn.training_accuracies)*batches_per_validate_data, batches_per_validate_data),\n",
    "                nn.training_accuracies, label=nn.name)\n",
    "    ax.set_xlabel('Training steps')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Validation Accuracy During Training')\n",
    "    ax.legend(loc=4)\n",
    "    ax.set_ylim([0,1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_and_test(use_larger_weights, learning_rate, activation_fn, training_batches=50000, batches_per_validate_data=500):\n",
    "    \"\"\"\n",
    "    使用相同的权重初始化生成两个网络对象，其中一个使用BN，另一个不使用BN\n",
    "\n",
    "    :param use_larger_weights: 是否使用更大的权重\n",
    "    :param learning_rate: 学习率\n",
    "    :param activation_fn: 激活函数\n",
    "    :param training_batches: 训练阶段使用的batch数（默认为50000）\n",
    "    :param batches_per_validate_data: 训练多少个batch后在validation数据上进行测试\n",
    "    \"\"\"\n",
    "    if use_larger_weights:\n",
    "        weights = [np.random.normal(size=(784,128), scale=10.0).astype(np.float32),\n",
    "                   np.random.normal(size=(128,128), scale=10.0).astype(np.float32),\n",
    "                   np.random.normal(size=(128,128), scale=10.0).astype(np.float32),\n",
    "                   np.random.normal(size=(128,10), scale=10.0).astype(np.float32)\n",
    "                   ]\n",
    "    else:\n",
    "        weights = [np.random.normal(size=(784,128), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(128,128), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(128,128), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(128,10), scale=0.05).astype(np.float32)\n",
    "                   ]\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    nn = NeuralNetWork(weights, activation_fn, use_batch_norm=False) # Without BN\n",
    "    bn = NeuralNetWork(weights, activation_fn, use_batch_norm=True) # With BN\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        print(\"【Training Result:】\\n\")\n",
    "\n",
    "        nn.train(sess, learning_rate, training_batches, batches_per_validate_data)\n",
    "        bn.train(sess, learning_rate, training_batches, batches_per_validate_data)\n",
    "\n",
    "        print(\"\\n【Testing Result:】\\n\")\n",
    "        nn.test(sess)\n",
    "        bn.test(sess)\n",
    "\n",
    "    plot_training_accuracies(nn, bn, batches_per_validate_data=batches_per_validate_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4.BN测试\n",
    "实验中，我们主要控制一下三个变量：\n",
    "- 权重矩阵（较小初始化权重，标准差为0.05；较大初始化权重，标准差为10）\n",
    "- 学习率（较小学习率：0.01；较大学习率：2）\n",
    "- 隐层激活函数（relu，sigmoid）\n",
    "\n",
    "### 5.4.1.较小的Weights初始化，较小学习率，relu激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=0.01, activation_fn=tf.nn.relu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以得到以下结论：\n",
    "- 在训练与预测阶段，加入BN的模型准确率都稍高一点；\n",
    "- 加入BN的网络收敛更快（黄线）\n",
    "- 没有加入BN的网络训练速度更快，这是因为BN增加了神经网络中的计算量\n",
    "\n",
    "为了更清楚地看到BN收敛速度更快，我们把减少Training batches，设置为3000，得到如下结果："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=0.01, activation_fn=tf.nn.relu, training_batches=3000, batches_per_validate_data=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "从上图中我们就可以清晰看到，加入BN的网络在第500个batch的时候已经能够在validation数据集上达到90%的准确率；而没有BN的网络的准确率还在不停波动，并且到第3000个batch的时候才达到90%的准确率。\n",
    "\n",
    "### 5.4.2.较小的Weights初始化，较小学习率，sigmoid激活函数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=0.01, activation_fn=tf.nn.sigmoid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "学习率与权重均没变，我们把隐层激活函数换为sigmoid。可以发现，BN收敛速度非常之快，而没有BN的网络前期在不断波动，直到第20000个train batch以后才开始进入平稳的训练状态。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.3.较小的Weights初始化，较大学习率，relu激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=2, activation_fn=tf.nn.relu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在本次实验中，我们使用了较大的学习率，较大的学习率意味着权重的更新跨度很大，而根据我们前面理论部分的介绍，BN不会受到权重scale的影响，因此其能够使模型保持在一个稳定的训练状态；而没有加入BN的网络则在一开始就由于学习率过大导致训练失败"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.4.较小的Weights初始化，较大学习率，sigmoid激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=2, activation_fn=tf.nn.sigmoid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在保持较大学习率（learning rate=2）的情况下，当我们将激活函数换为sigmoid以后，两个模型都能够达到一个很好的效果，并且在test数据及上的准确率非常接近；但加入BN的网络要收敛地更快，同样的，我们来观察3000次batch的训练准确率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=False, learning_rate=2, activation_fn=tf.nn.sigmoid, training_batches=3000, batches_per_validate_data=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当我们把training batch限制到3000以后，可以发现加入BN后，尽管我们使用较大的学习率，其仍然能够在大约500个batch以后在validation上达到90%的准确率；但不加入BN的准确率前期在一直大幅度波动，到大约1000个batch以后才达到90%的准确率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.5.较大的Weights初始化，较小学习率，relu激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=True, learning_rate=0.01, activation_fn=tf.nn.relu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当我们使用较大权重时，不加入BN的网络在一开始就失效；而加入BN的网络能够克服如此bad的权重初始化，并达到接近80%的准确率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.6.较大的Weights初始化，较小学习率，sigmoid激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=True, learning_rate=0.01, activation_fn=tf.nn.sigmoid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "同样使用较大的权重初始化，当我们激活函数为sigmoid时，不加入BN的网络在一开始的准确率有所上升，但随着训练的进行网络逐渐失效，最终准确率仅有30%；而加入BN的网络依旧出色地克服如此bad的权重初始化，并达到接近85%的准确率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.7.较大的Weights初始化，较大学习率，relu激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=True, learning_rate=2, activation_fn=tf.nn.relu)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当权重与学习率都很大时，BN网络开始还会训练一段时间，但随后就直接停止训练；而没有BN的神经网络开始就失效。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.8.较大的Weights初始化，较大学习率，sigmoid激活函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test(use_larger_weights=True, learning_rate=2, activation_fn=tf.nn.sigmoid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，加入BN对较大的权重与较大学习率都具有非常好的鲁棒性，最终模型能够达到93%的准确率；而未加入BN的网络则经过一段时间震荡后开始失效。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python36964bitanaconda3virtualenv5d618e8c7b894374a565943176146b80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}