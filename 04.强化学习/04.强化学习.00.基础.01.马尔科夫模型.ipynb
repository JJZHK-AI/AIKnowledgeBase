{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "马尔科夫模型\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.马尔科夫性质\n",
    "## 1.1.马尔科夫性\n",
    "系统的下一个状态只与当前状态有关，与以前状态无关"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2.定义\n",
    "一个状态$S_t$是马尔科夫的，当且仅当\n",
    "$$\\widetilde{P}\\[S_{t+1}|S_t]=\\widetilde{P}\\[S_{t+1}|S_1,...,S_t]$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3.特点\n",
    "- 当前状态蕴含所有相关的历史信息\n",
    "- 一旦当前状态已知，历史信息将会被抛弃\n",
    "- 状态$S_t$包含了所有历史相关信息，或者说历史的所有状态的相关信息都在当前状态$S_t$上体现出来，一旦$S_t$知道了，那么前面的$S_1,S_2,...,S_{t-1}$都可以抛弃了\n",
    "- 比如下棋时，只用关心当前局面，打俄罗斯方块时，只用关心当前屏幕\n",
    "- 有了马尔科夫状态之后，就可以定义状态转移矩阵，而且可以忽略时间的影响"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.状态转移矩阵\n",
    "状态转移概率是指从一个马尔科夫状态$s$跳转到候机状态$s'$的概率，有\n",
    "$$\\mathcal{P}_{ss'}=\\widetilde{P}\\[S_{t+1}=s'|S_t=s]$$\n",
    "所有的状态组成行，所有的候机状态组成列，我们得到状态转移矩阵如下\n",
    "$$\\mathcal{P}=\\begin{bmatrix}\n",
    "\\mathcal{P}_{11}&\\cdots&\\mathcal{P}_{1n}\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\mathcal{P}_{n1}&\\cdots&\\mathcal{P}_{nn}\n",
    "\\end{bmatrix}$$\n",
    "- n表示状态的个数\n",
    "- 由于$\\mathcal{P}$代表了整个状态转移的集合，所以用花体\n",
    "- 每行元素相加等于1"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将状态转移概率写成函数的形式\n",
    "$$\\mathcal{P}(s^'|s)=\\widetilde{P}\\[S_{t+1}=s^'|S_t=s]$$\n",
    "其中\n",
    "- $\\sum_{s^'}\\mathcal{P}(s^'|s)=1$\n",
    "- 状态数量太多或者是无穷大(连续状态)是，更适合使用状态转移函数，此时$\\int_{s^'}\\mathcal{P}(s^'|s)=1$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.马尔科夫过程(Markov process, MP)\n",
    "## 3.1.定义\n",
    "一个马尔科夫过程是一个无记忆的随机过程，即一些马尔科夫状态的序列。该过程中所有的状态均满足马尔科夫性."
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "马尔科夫过程可以由它可以表示为一个二元组$<\\mathcal{S}, \\mathcal{P}>$。它包含了状态集和状态转移矩阵。\n",
    "- $\\mathcal{S}$：状态集\n",
    "- $\\mathcal{P}$:状态转移矩阵$\\machcal{P}_{ss^'}=\\widetilde{P}\\[S_{t+1}=s^'|S_t=s]$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "虽然我们有时候并不知道$\\mathcal{P}$的具体值，但是通常我们假设$\\mathcal{P}$是存在并且稳定(即跟时间没有关系)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.马尔科夫过程举例\n",
    "![images](Images/04/00/01_005.png)\n",
    "- 一个学生每天需要学习三个科目，然后通过测验\n",
    "- 不过也有可能只学完两个科目之后直接睡觉\n",
    "- 一旦挂科有可能需要重新学习某些科目\n",
    "- 用椭圆表示普通状态，每一条线上的数字表示从一个状态跳转到另一个状态的概率\n",
    "- 方块表示终止(terminal)状态终止状态的定义有两种：时间中止、状态终止\n",
    "- 由于马尔可夫过程可以用图中的方块和线条组成，所以可以称马尔可夫过程为马尔可夫链(MDPs chain)\n",
    "## 3.3.片段\n",
    "强化学习中，从初始状态$S_1$到终止状态的序列过程，被称为一个片段(episode)\n",
    "$$S_1,S_2,...,S_T$$\n",
    "- 如果一个任务总以终止状态结束，那么这个任务被称为片段任务(episodic task)\n",
    "- 如果一个任务会没有终止状态，会被无限执行下去，这被称为连续性任务(continuing task)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.马尔科夫奖励过程(Markov Reward Process, MRP)\n",
    "## 4.1.定义\n",
    "当马尔科夫过程中加入奖励和折扣因子后，就变成了马尔科夫奖励过程$M=<\\mathcal{S},\\mathcal{P},\\mathcal{R},\\gamma>$\n",
    "- \\mathcal{S}:状态集。随机变量序列$\\{S_t\\}_{t=0,1,2,...}$。$S_t$表示机器人第t步所在的位置，也就是状态\n",
    "- \\mathcal{P}:状态转移矩阵，但是并不涉及奖励值。$P_r(S_{t+1}|S_t,a_t)$满足马尔科夫性。\n",
    "- $\\gamma$:折扣因子(也叫衰退系数)，是一个$\\[O~l]$之间的数。一般根据时间的延长作用越来越小，表明越远的奖励对当前的贡献越少。\n",
    "- \\mathcal{R}:奖励值或者是奖励值函数。每一个状态对应一个值，或者一个状态－动作对对应于一个奖励值。奖励函数$\\mathcal{R}(s)=\\widetilde{E}\\[R_{t+1}|S_t=s]$。$\\widetilde{E}$表示期望"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2.举例\n",
    "将上述例子加入奖励值，得到下图\n",
    "![images](Images/04/00/01_006.png)\n",
    "- 奖励值：对每一个状态的评价，奖励值不是强化学习的目标，我们的目标是回报值\n",
    "- 回报值：对每一个片段的评价"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3.回报值\n",
    "回报值(return $G_t$)是从时间$t$处开始的累计衰减奖励\n",
    "- 对于片段任务有:$G_t=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{T-t-1}R_T=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$\n",
    "- 对于连续性任务:$G_t=R_{t+1}+\\gamma R_{t+2}+...=\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}$\n",
    "![images](Images/04/00/01_007.png)\n",
    "- 终止状态等价于自身转移概率为1，奖励为0的的状态\n",
    "- 因此我们能够将片段性任务和连续性任务统一表达\n",
    "$$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$$\n",
    "这里当$T=\\infty$时，表示连续性任务，否则即为片段性任务"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4.衰减值\n",
    "使用指数衰减的原因\n",
    "### 4.4.1.直观感觉\n",
    "- 影响未来的因素不仅仅包含当前\n",
    "- 我们对未来的把握也是逐渐衰减的\n",
    "- 一般情况下，我们更关注短时间的反馈\n",
    "### 4.4.2.数学便利\n",
    "- 一个参数就描述了整个衰减过程，只需要调节这一个参数$\\gamma$即可以调节长时奖励和短时奖励的权衡(trade-off)\n",
    "- 指数衰减形式又很容易进行数学分析\n",
    "- 指数衰减是对回报值的有界保证，避免了循环MRP和连续性MRP情况下回报值变成无穷大"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5.MRP值函数\n",
    "### 4.5.1.为什么需要值函数\n",
    "- 回报值是一次片段的结果，存在很大的样本偏差\n",
    "- 回报值的角标是$t$，值函数关注的是状态$s$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5.2.定义\n",
    "一个MRP的值函数如下定义\n",
    "$v(s)=\\widetilde{E}\\[G_t|S_t=s]$\n",
    "- 这里的值函数针对的是状态$s$，所以称为状态值函数，又称V函数\n",
    "- $G_t$是一个随机变量\n",
    "- 这里使用小写的$v$函数，代表了真实存在的值函数"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6.值函数与回报值举例\n",
    "依然使用上面的例子，针对初始状态$S_1$=“科目一”,且$\\gamma=\\frac{1}{2}$计算不同片段的回报值\n",
    "- “科目一”，“科目二”，“睡觉”\n",
    "$$g_1=-2+\\frac{1}{2} \\times -2=-3$$\n",
    "- “科目一”，“科目二”，“科目三”，“通过”，“睡觉”\n",
    "$$g_1=-2+\\frac{1}{2} \\times -2+\\frac{1}{2}^2 \\times -2+\\frac{1}{2}^3 \\times 10=-2.25$$\n",
    "“科目一”，“玩手机”，“玩手机”, “玩手机”，“科目一”, “科目二”, “睡觉”\n",
    "$$g_1=-2+\\frac{1}{2} \\times -1+\\frac{1}{2}^2 \\times -1+\\frac{1}{2}^3 \\times -1+\\frac{1}{2}^4 \\times -2+\\frac{1}{2}^5 \\times -2=-3.0625$$\n",
    "- “科目一”，“科目二”，“科目三”，“挂科”，“科目二”，“科目三”，“挂科”，“科目一”，“科目二”，“科目三”，“挂科”, “科目三”，“通过”, “睡觉”\n",
    "$$g_1=-2+\\frac{1}{2} \\times -2+\\frac{1}{2}^2 \\times -2+...=-4.42138671875$$\n",
    "\n",
    "虽然都是从相同的初始状态开始，但是不同的片段有不同的回报值，而值函数是它们的期望值。\n",
    "![images](Images/04/00/01_008.png)\n",
    "![images](Images/04/00/01_009.png)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.7.贝尔曼方程\n",
    "$$\\begin{split}\n",
    "v(s)&=\\widetilde{E}\\[G_t|S_t=s]\\\\\n",
    "&=\\widetilde{E}\\[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+...|S_t=s] \\\\\n",
    "&=\\widetilde{E}\\[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+...)|S_t=s] \\\\\n",
    "&=\\widetilde{E}\\[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "&=\\widetilde{E}\\[R_{t+1}+\\gamma v(S_{t+1})|S_t=s] \n",
    "\\end{split}$$\n",
    "- 体现了$v(s)$与$v(S_{t+1})之间的迭代关系\n",
    "- 这是强化学习中的理论核心之一\n",
    "- 注意$s$小写，$S_{t+1}大写"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果我们已知转移矩阵$\\mathcal{P}$，那么\n",
    "$$\\begin{split}\n",
    "v(s)&=\\widetilde{E}\\[R_{t+1}+\\gamma v(S_{t+1})|S_t=s] \\\\\n",
    "&=\\widetilde{E}\\[R_{t+1}|S_t=s] + \\gamma\\widetilde{E}\\[v(S_{t+1})|S_t=s] \\\\\n",
    "&=\\mathcal{R}(s)+\\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{P}_{ss^'}v(s^')\n",
    "\\end{split}$$\n",
    "![images](Images/04/00/01_010.png)\n",
    "图中我们可以用$v(s_1),v(s_2),v(s_3)$去计算$v(s_0)$\n",
    "![images](Images/04/00/01_011.png)\n",
    "当$\\gamma=1$的时候，可以验算一下是否符合贝尔曼方程。对于科目三来说，本身的奖励值是$-2$，然后它有$0.6$的概率跳转到\"通过\"状态，有$0.4$的概率跳转到\"挂科\"，所以有\n",
    "$$g_1=-2 + 0.6 \\times 10 + 0.4 \\times -8.5=0.6$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.8.求解贝尔曼方程\n",
    "我们可以使用矩阵向量的形式表达贝尔曼方程，即\n",
    "$$v=\\mathcal{R}+\\gamma\\mathcal{P}v$$\n",
    "有\n",
    "$$\\begin{bmatrix}\n",
    "v(s_1)\\\\\n",
    "\\vdots\\\\\n",
    "v(s_n)\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\mathcal{R}(s_1)\\\\\n",
    "\\vdots\\\\\n",
    "\\mathcal{R}(s_n)\n",
    "\\end{bmatrix} + \\gamma \\times \\begin{bmatrix}\n",
    "\\mathcal{P}_{s_1s_1}&\\cdots&\\mathcal{P}_{s_1s_n}\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\mathcal{P}_{s_ns_1}&\\cdots&\\mathcal{P}_{s_ns_n}\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "v(s_1)\\\\\n",
    "\\vdots\\\\\n",
    "v(s_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "贝尔曼方程本质上是一个线性方程，可以直接求解\n",
    "$$\\begin{split}\n",
    "v&=\\mathcal{R}+\\gamma\\mathcal{P}v \\\\\n",
    "(I-\\gamma\\mathcal{P})v&=\\mathcal{R} \\\\\n",
    "v&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n",
    "\\end{split}$$\n",
    "- 计算复杂度O(n3)\n",
    "- 要求已知状态转移矩阵$\\mathcal{P}$\n",
    "- 直接求解的方式仅限于小的MRPs"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.马尔科夫决策过程(MDP)\n",
    "马尔科夫决策过程是强化学习的基础，激活所有的问题都通过马尔科夫决策过程来描述。对于马尔科夫过程和马尔科夫奖励过程中，我们都是作为观察者去观察其中的状态转移现象，去计算回报值"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1.MDP的定义\n",
    "在马尔科夫奖励过程中引入鞠策，就成了马尔科夫决策过程。马尔科夫决策过程定义为$<\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma>$\n",
    "- S:有限状态集\n",
    "- A:有限动作集。\n",
    "- P:状态转移矩阵\n",
    "$$\\mathcal{P}_{ss^'}^a=\\widetilde{P}\\[S_{t+1}=s^'|S_t=s,A_t=a]$$\n",
    "- $\\gamma$:折扣因子(也叫衰退系数)，是一个$O~l$之间的数。一般根据时间的延长作用越来越小，表明越远的奖励对当前的贡献越少。\n",
    "- R:奖励值或者是奖励值函数。每一个状态对应一个值，或者一个状态－动作对对应于一个奖励值。奖励函数$R(S,a)$表示状态$S$下执行行动a后可以得到的即时奖励的期望\n",
    "$$\\mathcal{R}(s,a)=\\widetilde{E}\\[R_{t+1}|S_t=s,A_t=a]$$\n",
    "![images](Images/04/00/01_012.png)\n",
    "- 针对状态的奖励变成了针对$<s;a>$的奖励\n",
    "- 能通过动作进行控制的状态转移由原来的状态转移概率替换成了动作\n",
    "- MDP只关注哪些可以做出决策的动作\n",
    "- 被动的状态转移关系被压缩成一个状态。“被动” 指无论做任何动作，状态都会发生跳转。这样的状态不属于MDPs中考虑的状态\n",
    "- 原图中，由于“通过” 后一定去“睡觉”，因此进行了压缩。原图中，“挂科” 后的跳转不受控制\n",
    "\n",
    "注: 图中除了在“科目三” 的状态上执行“不复习” 动作外，其他的所有状态跳转都是确定性的，我们通过在不同的状态上执行不同的动作，即可实现不同的状态跳转"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2.策略\n",
    "在MDP中，一个策略$\\pi$是在给定状态下的动作的概率分布\n",
    "$$\\pi(a|s)=\\widetilde{P}\\[A_t=a|S_t=s]$$\n",
    "策略有如下特点\n",
    "- 策略是对智能体行为的全部描述\n",
    "- MDPs中的策略是基于马尔可夫状态的(而不是基于历史)\n",
    "- 策略是时间稳定的，只与$s$有关，与时间$t$无关\n",
    "- 策略是RL问题的终极目标\n",
    "- 如果策略的概率分布输出都是one-hot的，那么称为确定性策略，否则即为随机策略"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3.MDP和MRP的关系\n",
    "对于一个MDP问题$<\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma>$，如果给定了策略$\\pi$，那么\n",
    "- MDP将会退化成一个MRP问题，$<\\mathcal{S},\\mathcal{P}^{\\pi},\\mathcal{R}^{\\pi},\\gamma>$\n",
    "- 此时有：\n",
    "$$\\begin{split}\n",
    "\\mathcal{P}_{ss'}^{\\pi}&=\\sum_{a \\in \\mathcal{A}}\\pi(a|s)\\mathcal{P}_{ss'}^a \\\\\n",
    "\\mathcal{R}_s^{\\pi}&=\\sum_{a \\in \\mathcal{A}}\\pi(a|s)\\mathcal{R}_s^a\n",
    "\\end{split}$$\n",
    "注意由于策略$\\pi$代表确定策略(一个动作)或随机策略(一个动作分布)，在确定策略的时候，$a=\\pi$.所以在有时,$\\pi$和$a$会存在一定的混淆。"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4.MDP的值函数\n",
    "在MDP问题中，由于动作的引入，值函数分为了两种：状态值函数(V函数)和状态动作值函数(Q函数)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.1.状态值函数\n",
    "MDP中的状态值函数是从状态$s$开始，使用策略$\\pi$得到的期望回报值\n",
    "$$v_{\\pi}(s)=\\widetilde{E}_{\\pi}\\[G_t|S_t=s]$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2.状态动作值函数\n",
    "MDP中的状态动作值函数是从状态$s$开始，执行动作$a$，然后使用策略$\\pi$得到的期望回报值\n",
    "$$q_{\\pi}(s,a)=\\widetilde{E}\\[G_t|S_t=s,A_t=a]$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5.贝尔曼期望方程\n",
    "和MRP相似，MDP中的值函数也能分解成瞬时奖励和后继状态的值函数两部分\n",
    "$$\\begin{split}\n",
    "v_{\\pi}(s)&=\\widetilde{E}_{\\pi}\\[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s] \\\\\n",
    "q_{\\pi}(s,a)&=\\widetilde{E}\\[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]\n",
    "\\end{split}$$\n",
    "![images](Images/04/00/01_013.png)\n",
    "当$\\pi(a|s)=0.5,\\forall a,s$且$\\gamma=1$时，$v_{\\pi}(s)$如上图。将图中的圆圈状态从上到下从左到右分别记为$s_1,s_2,s_3,s_4$，将终止状态记为$s_5$，则当$\\pi(a|s)=0.5,\\forall a,s$时，状态转移矩阵为\n",
    "![images](Images/04/00/01_014.png)\n",
    "给定了策略$\\pi$之后，就可以退化到MRP"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6.V函数与Q函数的关系\n",
    "![images](Images/04/00/01_015.png)\n",
    "$$v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_{\\pi}(s,a)$$\n",
    "本质是全概率公式\n",
    "![images](Images/04/00/01_016.png)\n",
    "$$q_{\\pi}(s,a)=\\mathcal{R}(s,a)+\\gamma\\sum_{s' \\in S}\\mathcal{P}_{ss'}^av_{\\pi}(s')$$\n",
    "我们可以看到当前的V函数跟当前的Q函数有一个关系，当前的Q函数又跟下一状态的V函数有一个关系，所以当前的V函数跟下一状态的V函数也有一个关系，级联起来就有\n",
    "![images](Images/04/00/01_017.png)\n",
    "$$v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_{\\pi}(s,a)=\\sum_{a \\in \\mathcal{A}}\\pi(a|s)\\lgroup \\mathcal{R}(s,a)+\\gamma\\sum_{s' \\in S}\\mathcal{P}_{ss'}^av_{\\pi}(s') \\rgroup$$\n",
    "实际上等价于$v_{\\pi}(s)=\\widetilde{E}_{\\pi}\\[R_{t+1}+\\gamma v_{pi}(S_{t+1})|S_t=s]$\n",
    "对于Q函数也有\n",
    "![images](Images/04/00/01_018.png)\n",
    "当$\\gamma=1,\\pi(a|s)=0.5$时，有\n",
    "![images](Images/04/00/01_019.png)\n",
    "$$2.8=0.5 \\times (10+0) + 0.5 \\times (-5+0.2 \\times -3.6 + 0.4 \\times 0.4 + 0.4 \\times 2.8)$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.7.贝尔曼方程的矩阵形式\n",
    "MDP下的贝尔曼期望方程和MRP下的形式相同\n",
    "贝尔曼方程本质上是一个线性方程，可以直接求解\n",
    "$$\\begin{split}\n",
    "v_{\\pi}&=\\mathcal{R}^{\\pi}+\\gamma\\mathcal{P}^{\\pi}v_{\\pi} \\\\\n",
    "v_{\\pi}&=(I-\\gamma\\mathcal{P}^{\\pi})^{-1}\\mathcal{R}^{\\pi}\n",
    "\\end{split}$$\n",
    "求解的要求\n",
    "- 已知$\\pi(a|s)$\n",
    "- 已知$\\mathcal{P}_{ss'}^a$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.8.最优值函数\n",
    "之前值函数，以及贝尔曼期望方程针对的都是给定策略$\\pi$的情况，是一个评价的问题。现在我们来考虑强化学习中的优化问题，即找出最好的策略。"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.8.1.定义\n",
    "最优值函数指的是在所有策略中的值函数最大值，其中包括最优V函数和最优Q函数\n",
    "$$\\begin{split}\n",
    "v_\\*(s)&=max_{\\pi}v_{\\pi}(s) \\\\\n",
    "q_\\*(s,a)&=max_{\\pi}q_{\\pi}(s,a)\n",
    "\\end{split}$$\n",
    "- 最优值函数指的是一个MDP中所能达到的最佳性能\n",
    "- 如果我们找到最优值函数即相当于这个MDP已经解决了"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.8.2.最优V函数\n",
    "![images](Images/04/00/01_020.png)\n",
    "当$\\gamma=1$时的最优V函数"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.8.3.最优Q函数\n",
    "![images](Images/04/00/01_021.png)\n",
    "当$\\gamma=1$时的最优Q函数"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.9.最优策略\n",
    "对于任何MDP问题\n",
    "- 总存在一个策略$\\pi_\\*$要好于或等于其他所有的策略,$\\pi_\\* \\geq \\pi, \\forall \\pi$\u0019\n",
    "- 所有的最优策略都能够实现最优的V函数$v_{\\pi_\\*}(s)=v(s)$\n",
    "- 所有的最优策略都能够实现最优的Q函数$q_{\\pi_\\*}(s;a)=q(s;a)$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "但我们已知了最优Q函数后，我们能够马上求出最优策略，只要根据$q_\\*(s;a)$选择相应的动作即可.可以看出对于任何MDPs 问题，总存在一个确定性的最优策略\n",
    "![images](Images/04/00/01_022.png)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.10.$v_\\*$和$q_\\*$相互转换\n",
    "$$v_\\*(s)=max_aq_\\*(s,a)$$\n",
    "\n",
    "| 贝尔曼最优方程V函数 | 贝尔曼最优方程Q函数 |\n",
    "| ---------------- | ---------------- |\n",
    "| ![images](Images/04/00/01_023.png) | ![images](Images/04/00/01_024.png) |\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 5.11.贝尔曼最优方程和贝尔曼期望方程的关系\n- 贝尔曼最优方程本质上就是利用了$\\pi_\\*$的特点，将求期望的算子转化成了$max_a$\n- 在贝尔曼期望方程中,$\\pi$是已知的。而在贝尔曼最优方程中,$\\pi_\\*$是未知的\n- 解贝尔曼期望方程的过程即对应了评价，解贝尔曼最优方程的过程即对应了优化\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}