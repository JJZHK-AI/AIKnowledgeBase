{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理之文本特征处理\n",
    "===\n",
    "- 文本特征处理包括为语料添加具有普适性的文本特征，如n-gram特征，以及对加入特征在后的文本语料进行必要的处理，如长度规范。这些特征处理工作能够有效的将重要的文本特征加入模型训练中，增强模型评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.添加N-gram特征\n",
    "给定一段文本序列，其中n个词或字的相邻共现特征即n-gram特征，常用的n-gram特征是bigram和tri-gram特征，分别对应n为2和3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.定义\n",
    "语言模型用来判断是否一句话从语法上是通顺的，具体办法就是求解整句话的概率。有一个句子s，其中s是由很多单词构成的:$\\omega_1,\\omega_2,...,\\omega_n$，我们需要计算在这个语言模型下这个句子的概率，即"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(s)=p(\\omega_1,\\omega_2,...,\\omega_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如有一句话\"全民AI是趋势\"，首先经过分词，得到\"全民，AI，是，趋势\"这些词，那么就有\n",
    "$$P_{LM}(全民AI是趋势)=P_{LM}(全民，AI，是，趋势)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.计算方法-Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A,B,C,D)=P(A) \\times P(B|A) \\times P(C|A,B) \\times P(D|A,B,C)$$\n",
    "$$P(\\omega_1,\\omega_2,...,\\omega_n)=P(\\omega_1) \\times P(\\omega_2|\\omega_1) \\times ... \\times P(\\omega_n|\\omega_1,\\omega_2,...,\\omega_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例来说，$P(地位|京东，AI，技术，处于，领先)$，它的意思是，在语料库中，出现京东AI技术处于领先这句话的同时，后面一个词是\"地位\"的概率，比如我们的语料库中有两个地方出现了\"京东AI技术处于领先\"，其中一个地方的后面的词是“地位”，另一个后面的词是“的”，那么它的概率就是$\\frac{1}{2}$。但是“京东AI技术处于领先”这句话太长了，很有可能在语料库中根本就没出现过，那么概率就是0了，这就导致了稀疏性。于是我们就会使用马尔科夫的假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.马尔科夫假设与N-gram\n",
    "$P(地位|京东，AI，技术，处于，领先)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1.阶马尔科夫假设-Unigram Model(1-gram Model)\n",
    "假定上述概率约等于$P(地位)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2.阶马尔科夫假设-Bigram Model(2-gram Model)\n",
    "假定上述概率约等于$P(地位|京东)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3.阶马尔科夫假设-Trigram Model(3-gram Model)\n",
    "假定上述概率约等于$P(地位|京东，AI)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$P(\\omega_1,\\omega_2,...\\omega_n)$来说，如果使用1-gram Model,就变成了\n",
    "$$P(\\omega_1) \\times P(\\omega_2) \\times ... \\times P(\\omega_n)=\\prod_{i=1}^nP(\\omega_i)$$\n",
    "如果使用2-gram Model，就变成了\n",
    "$$P(\\omega_1) \\times P(\\omega_2|\\omega_1) \\times ... \\times P(\\omega_n|\\omega_{n-1})=P(\\omega_1)\\prod_{i=2}^nP(\\omega_i|\\omega_{i-1})$$\n",
    "如果使用3-gram Model，就变成了\n",
    "$$P(\\omega_1) \\times P(\\omega_2|\\omega_1) \\times ... \\times P(\\omega_n|\\omega_{n-2},\\omega_{n-1})=P(\\omega_1) \\times P(\\omega_2|\\omega_1)\\prod_{i=3}^nP(\\omega_i|\\omega_{i-2},\\omega_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一般使用2-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.举例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1.实例一\n",
    "假设有概率如下\n",
    "\n",
    "| 语料 | 概率 |\n",
    "| --- | ---- |\n",
    "| p(是\\|今天) | 0.01 |\n",
    "| p(今天) | 0.002 |\n",
    "| p(周日\\|是) | 0.001 |\n",
    "| p(周日\\|今天) | 0.0001 |\n",
    "| p(周日) | 0.02 |\n",
    "| p(是\\|周日) | 0.0002 |\n",
    "\n",
    "比较：今天是周日 VS 今天周日是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(今天，是，周日)=P_{LM}(今天) \\times P_{LM}(是|今天) \\times P_{LM}(周日|是)=0.002 \\times 0.01 \\times 0.001 = 2 \\times 10^{-8}$$\n",
    "$$P(今天，周日，是)=P_{LM}(今天) \\times P_{LM}(周日|今天)) \\times P_{LM}(是|周日)=0.002 \\times 0.0001 \\times 0.0002 = 4 \\times 10^{-11}$$\n",
    "所以第一句话更符合语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5.举例\n",
    "假设给定分词列表:[\"是谁\", \"敲动\", \"我心\"]，对应的数值映射列表为:[1,34,21]。我们可以认为数值映射列表中的每个数字是词汇特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = 2\n",
    "def create_ngram_set(input_list):\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_range)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 1), (1, 5), (5, 3), (3, 2), (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "input_list = [1,3,2,1,5,3]\n",
    "res = create_ngram_set(input_list)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.文本长度规范\n",
    "一般模型的输入需要等尺寸大小的矩阵，因此在进入模型前需要对每条文本数值映射后的长度进行规范，此时将根据句子长度分布分析出覆盖绝大多数文本的合理长度，对超长文本进行截断，对不足文本进行补齐(一般使用数字0)，这个过程就是文本长度规范"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "cutlen = 10\n",
    "def padding(x_train):\n",
    "    return sequence.pad_sequences(x_train, cutlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 32 55 63  2 21 78 32 23  1]\n",
      " [ 0  0  0  0  0  2 32  1 23  1]]\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1,23,5,32,55,63,2,21,78,32,23,1], [2,32,1,23,1]]\n",
    "res = padding(x_train)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}