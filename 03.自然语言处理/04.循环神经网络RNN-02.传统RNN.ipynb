{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统RNN\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.传统RNN的内部结构图\n",
    "![images](images/012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$t-1$时间步的输入是$X_{t-1}$，具体的公式如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_t=tanh[W_t(X_t, h_{t-1}) + b_t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Pytorch实现传统RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.3615, -0.0816,  0.4676,  0.0809, -0.0903,  0.7149],\n         [ 0.0134,  0.8895, -0.1791,  0.9140, -0.8770,  0.0802],\n         [ 0.3835,  0.8586,  0.6461, -0.4862,  0.3930,  0.6487]]],\n       grad_fn=<StackBackward>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# input_size : 5, 输入张量的维度\n",
    "# hidden_size : 6， 隐含层的维度，隐含层神经元的个数\n",
    "# number_layers : 1，隐含层的层数\n",
    "rnn = nn.RNN(5, 6, 1)\n",
    "\n",
    "# 输入序列的长度：1\n",
    "# 批次的样本数\n",
    "# 输入张量的维度\n",
    "input = torch.randn(1, 3, 5)\n",
    "h0 = torch.randn(1, 3, 6)\n",
    "output, hn = rnn(input, h0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.3615, -0.0816,  0.4676,  0.0809, -0.0903,  0.7149],\n         [ 0.0134,  0.8895, -0.1791,  0.9140, -0.8770,  0.0802],\n         [ 0.3835,  0.8586,  0.6461, -0.4862,  0.3930,  0.6487]]],\n       grad_fn=<StackBackward>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.传统RNN的优势\n",
    "由于内部结构简单，对计算资源要求低，相比之后我们要学习的RNN变体：LSTM和GRU模型参数总量少了很多，在短序列任务上性能和效果都表现优异。所谓短序列，就是指输入序列的长度比较短，基本在10以内"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.传统RNN的缺点\n",
    "传统RNN在解决长序列之间的关联时，通过实践，证明经典RNN表现很差，原因在于进行反向传播的时候，过长的序列导致梯度的计算异常，发生梯度消失或爆炸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}