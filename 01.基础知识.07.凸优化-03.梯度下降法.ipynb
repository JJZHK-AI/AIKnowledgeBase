{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "梯度下降法\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 1.基础概念\n",
    "梯度方向是函数值变化最快的方向\n",
    "- 全微分：考虑所有自变量变化时，函数值的变化情况\n",
    "- 偏微分：假设其它自变量不变，考虑一个变量变化，函数值的变化情况.$\\Delta{y}=\\sum_{i=1}^n\\frac{\\partial}{\\partial{x_i}}\\Delta{x_i}$\n",
    "- 梯度向量:所有变量偏微分组成的向量。梯度方向由L(θ)对θ的偏导数确定,所以我们需要沿着负梯度方向往下走\n",
    "\n",
    "## 1.1.计算梯度的两种方法\n",
    "还有一种方法就是在神经网络下的反向传播算法\n",
    "### 1.1.1.数值梯度-Numerical Gradient\n",
    "速度慢，但是简单，它是从梯度的定义出发来计算梯度的。\n",
    "$$\\frac{f(x+h)-f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "一个最基本的计算x点上f的梯度的算法\n",
    "f : 参数为x的一个函数\n",
    "x : 一个numpy的vector\n",
    "'''\n",
    "def eval_numerical_gradient(f,x):\n",
    "    fx = f(x) # 计算原始点上的函数值\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "    \n",
    "    # 对x的每一个维度都计算一遍\n",
    "    it = np.nditer(g, flag=['multi_index'], op_flag=['readwrite'])\n",
    "    while not it.finished:\n",
    "        \n",
    "        # 计算x+h处的函数值\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h\n",
    "        fxh = f(x)\n",
    "        x[ix] = old_value\n",
    "        \n",
    "        # 计算偏导数\n",
    "        grad[ix] = (fxh - fx) / h # 斜率\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### 1.1.2.解析梯度-Analytic Gradient\n",
    "速度快，但是更容易出错。首先需要求出来$\\frac{\\partial{f}}{\\partial{x}}$，然后在计算。可是有些复杂函数根本不能求偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 2.梯度下降法\n",
    "梯度下降法的思路就是找到负梯度方向，然后不停的以一个很小的间隔去向它移动，直到前后两次的高度差小于一定范围的时候，停止，这个时候可以求出m和b.梯度下降法更新参数的公式如下：\n",
    "$$\\begin{eqnarray}\n",
    "\\theta&:=&\\theta - \\alpha\\frac{\\partial}{\\partial{\\theta}}J(\\theta)\\\\\n",
    "\\theta_j&:=&\\theta_j-\\frac{1}{m}\\sum_{i=0}^m(\\theta_i*x_i-y_i)*x_i*\\alpha\n",
    "\\end{eqnarray}$$\n",
    "$\\alpha$表示学习率，一般定义为0.001，这个参数太大了，会在底部不停震荡，如果太小，那么迭代次数太多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 2.1.批量梯度下降算法:BGD\n",
    "这种方法使用整个数据集(the complete dataset)去计算代价函数的梯度。每次使用全部数据计算梯度去更新参数，批量梯度下降法会很慢，并且很难处理不能载入内存(don’t fit in memory)的数据集。在随机初始化参数后，按如下方式计算代价函数的梯度(参照梯度下降法更新参数的公式，m是所有样本的数量和)\n",
    "- 如果训练集有3亿条数据，你需要从硬盘读取全部数据到内存中；\n",
    "- 每次一次计算完求和后，就进行参数更新；\n",
    "- 然后重复上面每一步；\n",
    "- 这意味着需要较长的时间才能收敛；\n",
    "- 特别是因为磁盘输入/输出（disk I/O）是系统典型瓶颈，所以这种方法会不可避免地需要大量的读取。\n",
    "\n",
    "![images](../Results/01/01/07/03_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/randomcurve.txt' does not exist: b'data/randomcurve.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-42c6e384d487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxMat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myMat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxCopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myHat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxMat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myMat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxCopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myHat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBGD_Call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42c6e384d487>\u001b[0m in \u001b[0;36mloadDataSet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdataLoad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/randomcurve.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    695\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/randomcurve.txt' does not exist: b'data/randomcurve.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def loadDataSet():\n",
    "    dataLoad = pd.read_csv(\"data/randomcurve.txt\", header=None)\n",
    "    x = dataLoad.iloc[:, :2].values\n",
    "    y = dataLoad.values[:, 2]\n",
    "    return x, y\n",
    "\n",
    "def BGD(xArr, yArr, iter, alpha):\n",
    "    yArr = np.reshape(yArr, newshape=(-1,1))\n",
    "    mu, sigma = 0, 0.1  # 均值与标准差\n",
    "    weight_num = xArr.shape[1]\n",
    "    w = np.random.normal(mu, sigma, (weight_num,1))\n",
    "    m = xArr.shape[0]\n",
    "\n",
    "    for i in range(iter):\n",
    "        predict = xArr @ w\n",
    "        grad = (xArr.T @ (predict - yArr) / m) * alpha\n",
    "        w -= grad\n",
    "    return w\n",
    "\n",
    "def BGD_Call(x, y):\n",
    "    ws = BGD(x,y, 10000, 0.001)\n",
    "    xMat = np.mat(x)\n",
    "    yMat = np.mat(y)\n",
    "    xCopy = xMat.copy()\n",
    "    xCopy.sort(0)\n",
    "    yHat = xCopy * ws\n",
    "\n",
    "    return (xMat, yMat), (xCopy, yHat)\n",
    "\n",
    "x,y=loadDataSet()\n",
    "\n",
    "(xMat, yMat), (xCopy, yHat) = BGD_Call(x,y)\n",
    "plt.figure(facecolor='w')\n",
    "plt.scatter([xMat[:, 1].flatten()], [yMat.T[:, 0].flatten().A[0]], c='r')\n",
    "\n",
    "plt.plot(xCopy[:, 1], yHat, c='b')\n",
    "plt.grid(True)\n",
    "plt.title(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 2.2.随机梯度下降算法:SGD\n",
    "优先选择.每次拿到一个样本就开始梯度下降。批量梯度下降法被证明是一个较慢的算法，所以，我们可以选择随机梯度下降法达到更快的计算。随机梯度下降法的第一步是随机化整个数据集。在每次迭代仅选择一个训练样本去计算代价函数的梯度，然后更新参数。即使是大规模数据集，随机梯度下降法也会很快收敛。随机梯度下降法得到结果的准确性可能不会是最好的，但是计算结果的速度很快。在随机化初始参数之后，就开始计算梯度。如下为随机梯度下降法的伪码：(参照梯度下降法更新参数的公式，m是1)\n",
    "- 进入内循环（inner loop）;\n",
    "- 第一步：挑选第一个训练样本并更新参数，然后使用第二个实例；\n",
    "- 第二步：选第二个训练样本，继续更新参数；\n",
    "- 然后进行第三步…直到第n步；\n",
    "- 直到达到全局最小值\n",
    "\n",
    "随机梯度下降法不像批量梯度下降法那样收敛，而是游走到接近全局最小值的区域终止<br/>\n",
    "![images](../Results/01/01/07/03_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 2.3.mini-batch梯度下降算法:MBGD\n",
    "小批量梯度下降法是最广泛使用的一种算法，该算法每次使用一批(batch_size)训练样本（称之为一批）进行训练，能够更快得出准确的答案。小批量梯度下降法不是使用完整数据集，在每次迭代中仅使用m个训练样本去计算代价函数的梯度。一般小批量梯度下降法所选取的样本数量在50到256个之间，视具体应用而定(参照梯度下降法更新参数的公式，m是batch_size)\n",
    "- 这种方法减少了参数更新时的变化，能够更加稳定地收敛。\n",
    "- 同时，也能利用高度优化的矩阵，进行高效的梯度计算。\n",
    "\n",
    "![images](../Results/01/01/07/03_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 3.线性回归举例\n",
    "由于线性回归的损失函数是\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "两边求导，有\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial{\\theta_j}}&=&\\frac{\\partial}{\\partial{\\theta_j}}\\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2\\\\\n",
    "&\\Rightarrow& \\sum_{i=1}^m[2*\\frac{1}{2}(h_{\\theta}(x)-y) \\bullet \\frac{\\partial}{\\partial{\\theta_j}}(h_{\\theta}(x)-y)]\\\\\n",
    "&\\Rightarrow& \\sum_{i=1}^m[(h_{\\theta}(x)-y) \\bullet \\frac{\\partial}{\\partial{\\theta_j}}(\\sum_{i=0}^n\\theta_ix_i-y)]\\\\\n",
    "&\\Rightarrow& \\sum_{i=1}^m[(h_{\\theta}(x)-y) \\bullet x_j]\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "假设有n个特征，表达式如下\n",
    "$$h(\\Theta)=\\Theta_0+\\Theta_1x_1+\\Theta_2x_2+...+\\Theta_nx_n$$\n",
    "那么\n",
    "$$\n",
    "Repeat\\{\n",
    "   \\Theta_j := \\Theta_j - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_j^{(i)}, j={0,1,...,n}\n",
    "\\}\n",
    "$$\n",
    "有\n",
    "$$\\begin{cases}\n",
    "\\Theta_0 := \\Theta_0 - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_0^{(i)}\\\\\\\\\n",
    "\\Theta_1 := \\Theta_1 - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_1^{(i)}\\\\\\\\\n",
    "\\Theta_2 := \\Theta_2 - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^{(i)})-y^{(i)})x_2^{(i)}\\\\\\\\\n",
    "...\n",
    "\\end{cases}$$\n",
    "\n",
    "![images](../Results/01/01/07/03_004.png)\n",
    "这是什么意思，首先给$\\Theta_0,\\Theta_1,...\\Theta_n$设置初始值，基本都是1，然后中间部分一样，这个值就是用这些初始值带入m个向\n",
    "量，可以得到m个值，用这m个值分别减去它们对应的y值，然后用这个值分别乘以对应记录的当前向量特征的值($\\Theta_0$就是\n",
    "$X_0$，$X_0$没有，就是1，$\\Theta_1$就是当前记录的第一个特征的值)然后将这m个值求和,然后乘以步长，乘以$\\frac{1}{m}$,这个值我们可以叫\n",
    "它$\\delta$。对于$\\Theta_0$来说，他的下一个值就是$\\Theta_0-\\delta$，对于$\\Theta_1$来说，他的下一个值就是$\\Theta_1-\\delta$...直到前后两次的高度差小于一定范围。下面三幅图分别是order为3，6，9的情况下，梯度下降法(红色)和最小二乘法(绿色)的拟合曲线\n",
    "\n",
    "| order=3 | order=6 | order=9 |\n",
    "| ------- | ------- | ------- |\n",
    "| ![images](../Results/01/01/07/03_005.png) | ![images](../Results/01/01/07/03_006.png) | ![images](../Results/01/01/07/007.png) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}