{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python365jvsc74a57bd0e73f067f1c59be12cfda938665c6a960a5d3ec55ce3a05feea7c329cb1f7e9af",
   "display_name": "Python 3.6.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "传统RNN\n",
    "==="
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1.传统RNN的内部结构图\n",
    "![images](images/012.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "在$t-1$时间步的输入是$X_{t-1}$，具体的公式如下"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$$h_t=tanh[W_t(X_t, h_{t-1}) + b_t]$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2.Pytorch实现传统RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6b580c80af68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# input_size : 5, 输入张量的维度\n",
    "# hidden_size : 6， 隐含层的维度，隐含层神经元的个数\n",
    "# number_layers : 1，隐含层的层数\n",
    "rnn = nn.RNN(5, 6, 1)\n",
    "\n",
    "# 输入序列的长度：1\n",
    "# 批次的样本数\n",
    "# 输入张量的维度\n",
    "input = torch.randn(1, 3, 5)\n",
    "h0 = torch.randn(1, 3, 6)\n",
    "output, hn = rnn(input, h0)\n",
    "output"
   ]
  },
  {
   "source": [
    "# 3.传统RNN的优势\n",
    "由于内部结构简单，对计算资源要求低，相比之后我们要学习的RNN变体：LSTM和GRU模型参数总量少了很多，在短序列任务上性能和效果都表现优异。所谓短序列，就是指输入序列的长度比较短，基本在10以内"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4.传统RNN的缺点\n",
    "传统RNN在解决长序列之间的关联时，通过实践，证明经典RNN表现很差，原因在于进行反向传播的时候，过长的序列导致梯度的计算异常，发生梯度消失或爆炸"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}