{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-编码器\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/027.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由N个编码器层堆叠而成\n",
    "- 每个编码器层由两个子层连接结构组成\n",
    "- 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接\n",
    "- 第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.掩码张量\n",
    "掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.掩码张量的作用\n",
    "在transformer中, 掩码张量的主要作用在应用attention时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    # 生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵\n",
    "    # 在函数中, 首先定义掩码张量的形状\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, 再使其中的数据类型变为无符号8位整形unit8 \n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') # 上三角矩阵\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, \n",
    "    # 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, \n",
    "    # 如果是0, subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1, subsequent_mask中的该位置由1变成0 \n",
    "    return torch.from_numpy(1 - subsequent_mask) # 下三角矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 3],\n       [0, 5, 6],\n       [0, 0, 9],\n       [0, 0, 0]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=0)\n",
    "# 按照K的位置生成上三角矩阵，k=0表示在主对角线(1,5,9)处开始生成下三角矩阵，k=-1，表示向左下方移动一个位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm: tensor([[[1, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "sm = subsequent_mask(size)\n",
    "print(\"sm:\", sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f9858d8ea30>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 360x360 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.注意力\n",
    "我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.注意力计算规则\n",
    "它需要三个指定的输入$Q(query)$, $K(key)$,$V(value)$, 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如我们有一个问题: 给出一段文本，使用一些关键词对它进行描述!为了方便统一正确答案，这道题可能预先已经写出了一些关键词作为提示.其中这些给出的提示就可以看作是key， 而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，我们最终脑子里的value发生了变化，根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式，但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子， 使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制,需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.注意力机制\n",
    "注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/029.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.2594,  1.2167, -0.4878,  0.4976,  0.8593],\n        [-1.8731,  0.3004, -0.8478, -0.9652, -1.5069],\n        [ 1.4263,  1.0552, -1.3266,  0.1995,  2.6404],\n        [ 0.6596,  0.8923, -2.4125, -1.5468, -0.1726],\n        [ 0.5612,  0.0828, -1.2075,  0.4097, -0.3580]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.autograd.Variable(torch.randn(5, 5))\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.autograd.Variable(torch.zeros(5, 5))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.masked_fill(mask == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn: tensor([[[ 15.0258,   8.0364,  15.1160,  ...,  -3.3893,  11.6157,  -5.7277],\n",
      "         [-25.6366, -15.9603,  18.5502,  ...,   0.0000,  19.6376, -10.2258],\n",
      "         [  0.0000,   0.0000,  21.2937,  ..., -26.0253, -10.0401,  -2.6868],\n",
      "         [ 29.3873,   0.0000,  13.3779,  ...,  52.2231,  -9.2056, -33.0383]],\n",
      "\n",
      "        [[  1.6617,  24.7486, -11.6641,  ..., -35.0158,   0.0000,  17.3508],\n",
      "         [ 55.8104,  -5.6255,   4.1666,  ...,  -2.1684,  46.8155,  18.1351],\n",
      "         [ 93.1199,  55.2154,  47.2830,  ..., -30.7171,  -7.7014,  27.6765],\n",
      "         [ 33.9643, -20.3226, -28.1388,  ...,  16.3863,  -1.1394,  22.7751]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "p_attn: tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "from lib.PositionalEncoding import PositionalEncoding\n",
    "from lib.Embeddings import Embeddings\n",
    "from lib.Attention import attention\n",
    "\n",
    "d_model = 512 # 词嵌入维度是512维\n",
    "vocab = 1000# 词表大小是1000\n",
    "x = torch.autograd.Variable(torch.LongTensor([[100,2,421,508],[491,998,1,221]])) # 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4\n",
    "emb = Embeddings(d_model, vocab)\n",
    "embr = emb(x)\n",
    "\n",
    "dropout = 0.1 # 置0比率为0.1\n",
    "max_len=60 # 句子最大长度\n",
    "x = embr\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "query = key = value = pe_result\n",
    "attn, p_attn = attention(query, key, value)\n",
    "print(\"attn:\", attn)\n",
    "print(\"p_attn:\", p_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn: tensor([[[  4.6942,  -1.9810,  17.0844,  ...,   5.7021,   3.0019, -12.9197],\n",
      "         [  4.6942,  -1.9810,  17.0844,  ...,   5.7021,   3.0019, -12.9197],\n",
      "         [  4.6942,  -1.9810,  17.0844,  ...,   5.7021,   3.0019, -12.9197],\n",
      "         [  4.6942,  -1.9810,  17.0844,  ...,   5.7021,   3.0019, -12.9197]],\n",
      "\n",
      "        [[ 46.1391,  13.5040,   2.9117,  ..., -12.8788,   9.4937,  21.4844],\n",
      "         [ 46.1391,  13.5040,   2.9117,  ..., -12.8788,   9.4937,  21.4844],\n",
      "         [ 46.1391,  13.5040,   2.9117,  ..., -12.8788,   9.4937,  21.4844],\n",
      "         [ 46.1391,  13.5040,   2.9117,  ..., -12.8788,   9.4937,  21.4844]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "p_attn: tensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "query = key = value = pe_result\n",
    "\n",
    "# 令mask为一个2x4x4的零张量\n",
    "mask = torch.autograd.Variable(torch.zeros(2, 4, 4))\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "print(\"attn:\", attn)\n",
    "print(\"p_attn:\", p_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.多头注意力机制\n",
    "从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/030.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  2.6518,   5.5778,   0.0689,  ...,  -0.4410,  -2.3415,   5.0106],\n",
      "         [  0.3030,   2.3861,   0.5803,  ...,  -2.9160,   0.5175,   2.1776],\n",
      "         [  2.8040,   4.3131,  -0.7991,  ...,   0.3299,  -1.4044,   3.1119],\n",
      "         [  0.9429,  -0.9946,  -0.1604,  ...,   3.7631,  -4.1479,   1.9265]],\n",
      "\n",
      "        [[ -6.7589,  -9.6749,   3.9519,  ...,  -2.4641,   0.7034,   0.0604],\n",
      "         [ -4.2012, -11.4498,   2.9996,  ...,  -3.3334,  -2.1496,   2.5971],\n",
      "         [ -6.7732,  -9.7686,   4.3320,  ...,  -5.2262,   0.9100,  -2.6310],\n",
      "         [ -7.0776, -10.1681,   2.3223,  ...,  -1.7299,   0.6082,   4.5347]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "\n",
    "# 头数head\n",
    "head = 8\n",
    "\n",
    "# 词嵌入维度embedding_dim\n",
    "embedding_dim = 512\n",
    "\n",
    "# 置零比率dropout\n",
    "dropout = 0.2\n",
    "# 假设输入的Q，K，V仍然相等\n",
    "query = value = key = pe_result\n",
    "\n",
    "# 输入的掩码张量mask\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "mha = MultiHeadedAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "print(mha_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多头注意力机制:\n",
    "    - 每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头.将每个头的获得的输入送到注意力机制中, 就形成了多头注意力机制.\n",
    "- 多头注意力机制的作用:\n",
    "    - 这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.\n",
    "- 实现了多头注意力机制的类: MultiHeadedAttention\n",
    "    - 因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.\n",
    "    - clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.\n",
    "    - clones函数的输出是装有N个克隆层的Module列表.\n",
    "    - 接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.\n",
    "    - 它的实例化对象输入是Q, K, V以及掩码张量mask.\n",
    "    - 它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.前馈全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.前馈全连接层\n",
    "在Transformer中前馈全连接层就是具有两层线性层的全连接网络."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.前馈全连接层的作用\n",
    "考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2527,  0.3683,  1.3200,  ...,  0.9640, -0.6376, -0.4171],\n",
      "         [-0.2730,  1.3092,  2.2593,  ..., -0.5786, -0.0913, -0.2911],\n",
      "         [ 0.4396,  0.8270,  2.0878,  ...,  0.4099, -0.6176,  0.1170],\n",
      "         [-0.4021,  1.0899,  3.0212,  ..., -0.3452, -0.9432, -1.5087]],\n",
      "\n",
      "        [[ 2.6249, -0.0102,  0.7391,  ...,  2.5496, -1.7434, -3.6331],\n",
      "         [ 3.3073,  0.0708,  0.5323,  ...,  3.5287, -1.9094, -1.6386],\n",
      "         [ 2.4557,  0.6948, -0.6274,  ...,  3.6380, -2.1658, -1.1532],\n",
      "         [ 1.5157, -0.0956, -0.8481,  ...,  2.3237, -3.4699, -2.5981]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from lib.PositionwiseFeedForward import PositionwiseFeedForward\n",
    "d_model = 512\n",
    "# 线性变化的维度\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "x = mha_result\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "ff_result = ff(x)\n",
    "print(ff_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.规范化层\n",
    "它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4361,  0.4953,  1.5077,  ...,  1.1290, -0.5749, -0.3403],\n",
      "         [-0.2261,  1.2867,  2.1950,  ..., -0.5183, -0.0524, -0.2433],\n",
      "         [ 0.6258,  1.0902,  2.6019,  ...,  0.5902, -0.6416,  0.2391],\n",
      "         [-0.2523,  1.0061,  2.6352,  ..., -0.2043, -0.7087, -1.1856]],\n",
      "\n",
      "        [[ 1.6483,  0.0471,  0.5025,  ...,  1.6026, -1.0060, -2.1542],\n",
      "         [ 2.1735,  0.1175,  0.4107,  ...,  2.3142, -1.1404, -0.9684],\n",
      "         [ 1.5944,  0.4745, -0.3664,  ...,  2.3463, -1.3447, -0.7007],\n",
      "         [ 1.0015,  0.0057, -0.4593,  ...,  1.5008, -2.0796, -1.5408]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from lib.LayerNorm import LayerNorm\n",
    "\n",
    "features = d_model = 512\n",
    "eps = 1e-6\n",
    "x = ff_result\n",
    "ln = LayerNorm(features, eps)\n",
    "ln_result = ln(x)\n",
    "print(ln_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.子层连接结构\n",
    "输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/031.png)  ![images](images/032.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 15.1067,   7.8620,  15.2091,  ...,  -3.3893,  11.5510,  -5.8564],\n",
      "         [-25.5126, -16.1701,  18.5116,  ...,  -0.2279,  19.4917, -10.3415],\n",
      "         [  0.1472,  -0.2130,  21.3320,  ..., -26.1354, -10.1490,  -2.6868],\n",
      "         [ 29.4363,  -0.3045,  13.1850,  ...,  52.1794,  -9.3446, -33.1388]],\n",
      "\n",
      "        [[  1.7231,  24.7486, -11.6557,  ..., -35.2455,  -0.1286,  17.3508],\n",
      "         [ 55.8633,  -5.6491,   4.3722,  ...,  -2.3203,  46.6268,  18.3316],\n",
      "         [ 93.2670,  55.2894,  47.3759,  ..., -30.8440,  -7.6692,  27.6765],\n",
      "         [ 33.9643, -20.3598, -28.1388,  ...,  16.1753,  -1.2813,  23.0741]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "from lib.SublayerConnection import SublayerConnection\n",
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "# 令x为位置编码器的输出\n",
    "x = pe_result\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "\n",
    "# 假设子层中装的是多头注意力层, 实例化这个类\n",
    "self_attn =  MultiHeadedAttention(head, d_model)\n",
    "\n",
    "# 使用lambda获得一个函数类型的子层\n",
    "sublayer = lambda x: self_attn(x, x, x, mask)\n",
    "sc = SublayerConnection(size, dropout)\n",
    "sc_result = sc(x, sublayer)\n",
    "print(sc_result)\n",
    "print(sc_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.编码器层\n",
    "作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/033.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4645e+01,  7.6673e+00,  1.4996e+01,  ..., -3.2609e+00,\n",
      "           1.1972e+01, -5.9493e+00],\n",
      "         [-2.5577e+01, -1.6736e+01,  1.7808e+01,  ..., -4.7880e-01,\n",
      "           2.0479e+01, -1.0190e+01],\n",
      "         [ 4.2206e-03, -5.7985e-01,  2.1082e+01,  ..., -2.5989e+01,\n",
      "          -9.8385e+00, -2.7872e+00],\n",
      "         [ 2.9509e+01, -1.0259e-01,  1.3378e+01,  ...,  5.2219e+01,\n",
      "          -8.7093e+00, -3.2861e+01]],\n",
      "\n",
      "        [[ 1.0879e+00,  2.5385e+01, -1.2962e+01,  ..., -3.5016e+01,\n",
      "           3.4716e-01,  1.7283e+01],\n",
      "         [ 5.5810e+01, -5.5063e+00,  3.1003e+00,  ..., -1.4800e+00,\n",
      "           4.7385e+01,  1.8292e+01],\n",
      "         [ 9.3163e+01,  5.5126e+01,  4.7173e+01,  ..., -3.0988e+01,\n",
      "          -8.5355e+00,  2.7762e+01],\n",
      "         [ 3.3671e+01, -2.0333e+01, -2.8879e+01,  ...,  1.5715e+01,\n",
      "          -1.3606e+00,  2.2800e+01]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "from lib.PositionwiseFeedForward import PositionwiseFeedForward\n",
    "from lib.EncoderLayer import EncoderLayer\n",
    "size = 512\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "x = pe_result\n",
    "dropout = 0.2\n",
    "self_attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "\n",
    "el = EncoderLayer(size, self_attn, ff, dropout)\n",
    "el_result = el(x, mask)\n",
    "print(el_result)\n",
    "print(el_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.编码器\n",
    "编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8.0421e-01,  2.0448e-01,  6.9661e-01,  ..., -1.3006e-01,\n",
      "           5.0433e-01, -2.9646e-01],\n",
      "         [-8.4901e-01, -7.1930e-01,  8.3998e-01,  ...,  1.8372e-01,\n",
      "           8.8645e-01, -3.9564e-01],\n",
      "         [-5.5599e-03, -1.1633e-01,  8.6820e-01,  ..., -1.0176e+00,\n",
      "          -2.9405e-01, -1.7876e-01],\n",
      "         [ 1.3911e+00, -1.4005e-01,  4.4955e-01,  ...,  2.3076e+00,\n",
      "          -3.3113e-01, -1.5053e+00]],\n",
      "\n",
      "        [[ 1.4914e-01,  8.5155e-01, -3.6431e-01,  ..., -1.5363e+00,\n",
      "          -1.1590e-01,  6.4684e-01],\n",
      "         [ 2.7025e+00,  4.5692e-03,  4.2180e-01,  ...,  2.2123e-03,\n",
      "           2.0656e+00,  1.0070e+00],\n",
      "         [ 3.8703e+00,  2.2815e+00,  2.0192e+00,  ..., -1.3379e+00,\n",
      "          -2.4570e-01,  1.2335e+00],\n",
      "         [ 1.4576e+00, -8.9819e-01, -1.0635e+00,  ...,  6.9472e-01,\n",
      "          -1.5115e-01,  9.1170e-01]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "from lib.PositionwiseFeedForward import PositionwiseFeedForward\n",
    "from lib.EncoderLayer import EncoderLayer\n",
    "from lib.Encoder import Encoder\n",
    "import copy\n",
    "# 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数\n",
    "# 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.\n",
    "size = 512\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "dropout = 0.2\n",
    "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
    "\n",
    "# 编码器中编码器层的个数N\n",
    "N = 8\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "\n",
    "en = Encoder(layer, N)\n",
    "en_result = en(x, mask)\n",
    "print(en_result)\n",
    "print(en_result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}