{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理-文本张量的表示方法\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一段文本使用张量进行表示，其中一般将词汇表示成向量，称作词向量，再由各个词向量按顺序组成矩阵形成文本表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.One-Hot编码\n",
    "独热编码，将每个词表示成具有n个元素的向量，这个词向量中只有一个元素是1，其他元素都是0，不同词汇元素为0的位置不同，其中n的大小是整个语料中不同词汇的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "周杰伦 的One-Hot编码为: [1, 0, 0, 0, 0, 0]\n",
      "陈奕迅 的One-Hot编码为: [0, 1, 0, 0, 0, 0]\n",
      "王力宏 的One-Hot编码为: [0, 0, 1, 0, 0, 0]\n",
      "李宗盛 的One-Hot编码为: [0, 0, 0, 1, 0, 0]\n",
      "吴亦凡 的One-Hot编码为: [0, 0, 0, 0, 1, 0]\n",
      "鹿晗 的One-Hot编码为: [0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['./Tokenizer']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab = (\"周杰伦\", \"陈奕迅\", \"王力宏\", \"李宗盛\", \"吴亦凡\", \"鹿晗\")\n",
    "t = Tokenizer(num_words=None, char_level=False)\n",
    "t.fit_on_texts(vocab)\n",
    "\n",
    "for token in vocab:\n",
    "    zero_list = [0] * len(vocab)\n",
    "    token_index = t.texts_to_sequences([token])[0][0] - 1\n",
    "    zero_list[token_index] = 1\n",
    "    print(token, \"的One-Hot编码为:\", zero_list)\n",
    "\n",
    "tokenizer_path = \"./Tokenizer\"\n",
    "joblib.dump(t, tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李宗盛 的One-Hot编码为: [0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "t = joblib.load(tokenizer_path)\n",
    "\n",
    "token = \"李宗盛\"\n",
    "token_index = t.texts_to_sequences([token])[0][0] - 1\n",
    "zero_list = [0] * len(vocab)\n",
    "zero_list[token_index] = 1\n",
    "print(token, \"的One-Hot编码为:\", zero_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：操作简单，容易理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "劣势：完全割裂了词与词之间的关联，而且在大语料集下，每个向量的长度过大，占据大量内存，所以现在用这种办法很少了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Word2Vec\n",
    "这是一种流行的将词汇表示成词向量的无监督训练方法，该过程将构建神经网络模型，将网络参数作为词汇的词向量表示，它包含CBOW和skipgram两种训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.CBOW(Continuous bag of words，连续词袋模式)模式\n",
    "给定一段用于训练的文本语料，在选定某段长度(窗口)作为研究对象，使用上下词汇预测目标词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中(the quick brown fox jumps over the lazy dog)窗口大小为9，使用前后4个词汇对目标词汇进行预测。相当于我知道了上文的四个单词，也知道了下文的四个单词，来预测中间的Jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们给定的训练预料只有一句话:Hope can set you free，窗口大小为3，因此模型的第一个训练样本来自Hope can set，因为是CBOW模式，所以将使用Hope和Set作为输入，Can作为输出，在模型训练时，Hope、can、set等词汇都是用他们的one-hot编码，如图所示：每个one-hot编码的单词与各自的变换矩阵(即参数矩阵$3 \\times 5$这里的3指的是得到的词向量维度)相城之后再相加，得到上下文表示矩阵$3 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，将上下文表示矩阵与变换矩阵(参数矩阵$5 \\times 3$，所有的变换矩阵共享参数)相乘，得到$5 \\times 1$进行损失的计算，然后更新网络参数完成一次模型迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后窗口按序向后移动，重新更新参数，知道所有语料都被遍历完成，得到最终的变换矩阵$3 \\times 5$，这个变换矩阵与每个词汇的one-hot编码$5 \\times 1$相乘，得到的$3 \\times 1$的矩阵就是该词汇的word2vec张量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.Skip-Gram过程\n",
    "Skip-Gram与CBOW的过程正好相反，它是给定中间词，去预测上下文的别的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们给定的训练语料只有一句话:Hope can set you free，窗口大小为3，因此模型的第一个训练样本来自Hope can set，那么输入就是can，而输出就是hope与set。在模型训练时，Hope，can，set等词汇都是用它们的one-hot编码，如图所示，将can的one-hot编码与变换矩阵(即参数矩阵$3 \\times 5$，这里的3是指最后得到的词向量的维度)相乘，得到目标词汇表示矩阵$3 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，将目标词汇表示矩阵与多个变换矩阵$5 \\times 3$相乘，得到多个$5 \\times 1$的结果矩阵，它将与我们hope和set对应的one-hot编码矩阵进行损失的计算，然后更新网络参数完成一次模型迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后窗口按序向后移动，重新更新参数，知道所有语料遍历完成，得到最终的变换矩阵即参数矩阵$3 \\times 5$，这个变换矩阵与每个词汇的one-hot编码相乘，得到的$3 \\times 1$的矩阵就是该词汇的word2vec张量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Word Embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}