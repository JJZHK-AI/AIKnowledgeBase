{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU模型\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU(Gated Recurrent Unit)也称作门控循环单元结构，他也是传统RNN的变体，通LSTM一样能够有效捕捉长序列之间的语义关联，缓解梯度消失或爆炸现象，同时它的结构和计算要比LSTM更简单，它的核心结构可以分为两个部分去解析：更新门、重置门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.GRU的内部结构图与计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "z_t&=&\\sigma[W_z \\bullet (h_{t-1},x_t)] \\\\\n",
    "r_t&=&\\sigma[W_r \\bullet (h_{t-1},x_t)] \\\\\n",
    "\\tilde{h_t}&=&tanh[W \\bullet (r_t \\times h_{t-1}, x_t)] \\\\\n",
    "h_t&=&(1-z_t) \\times h_{t-1} + z_t \\times \\tilde{h_t}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_t$是更新门，$r_t$就是重置门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算更新门和重置门的门值，分别是$z_t$和$r_t$，计算方法就是使用$X_t$与$h_{t-1}$拼接进行线性变换在经过sigmoid激活\n",
    "- 更新门门值的左右在了$h_{t-1}$上，代表控制上一时间步传来的信息有多少可以被利用\n",
    "- 使用这个更新后的$h_{t-1}$进行基本的RNN计算，即与$x_t$拼接进行线性变换，经过tanh激活，得到新的$h_t$\n",
    "- 重置门的门值会作用在新的$h_t$，而1减去门值会作用在$h_{t-1}$，随后将两者的结果相加，得到最终的隐含状态输出$h_t$\n",
    "- 这个过程意味着重置门有能力重置之前所有的计算，当门值趋于1时，输出就是新的$h_t$，而当门值趋于0时，输出就是上一时间步的$h_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-GRU和Bi-LSTM的逻辑相同，都是不改变其内部结构，而是将模型应用两次且方向不同，再将两次得到的LSTM结果进行拼接作为最终输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.GRU在Pytorch中的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.2275, -0.9000, -0.6166,  0.4027,  0.5367, -0.0312],\n         [ 0.9569,  0.1598, -0.4007,  0.1350,  0.5063, -0.6303],\n         [-0.3321, -0.1651, -0.1089, -0.3715,  0.6901,  0.1050]]],\n       grad_fn=<StackBackward>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "rnn = nn.GRU(5, 6, 2)\n",
    "input = torch.randn(1, 3, 5)\n",
    "h0 = torch.randn(2, 3, 6)\n",
    "output, hn = rnn(input, h0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.9484, -0.6072,  0.0513,  1.4981,  0.4362,  0.3444],\n         [ 0.4520,  0.0805, -0.9564, -0.3014, -0.6984, -0.2426],\n         [-0.2173,  0.2282,  0.5268,  0.3826,  0.0648,  0.4860]],\n\n        [[ 0.2275, -0.9000, -0.6166,  0.4027,  0.5367, -0.0312],\n         [ 0.9569,  0.1598, -0.4007,  0.1350,  0.5063, -0.6303],\n         [-0.3321, -0.1651, -0.1089, -0.3715,  0.6901,  0.1050]]],\n       grad_fn=<StackBackward>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.GRU的优势和劣势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.优势\n",
    "GRU和LSTM作用相同，在捕捉长序列语义关联时，能有效抑制梯度消失或爆炸，效果都优于传统RNN且计算复杂度相比LSTM要小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.缺点\n",
    "GRU仍然不能完全解决梯度消失问题，同时其作用RNN的变体，有着RNN结构本身的一大弊端，即不可并行计算，这在数据量和模型体量逐步增大的未来，是RNN发展的关键瓶颈"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}