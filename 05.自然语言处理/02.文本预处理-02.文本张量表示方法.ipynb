{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理-文本张量的表示方法\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一段文本使用张量进行表示，其中一般将词汇表示成向量，称作词向量，再由各个词向量按顺序组成矩阵形成文本表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.One-Hot编码\n",
    "独热编码，将每个词表示成具有n个元素的向量，这个词向量中只有一个元素是1，其他元素都是0，不同词汇元素为0的位置不同，其中n的大小是整个语料中不同词汇的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "周杰伦 的One-Hot编码为: [1, 0, 0, 0, 0, 0]\n",
      "陈奕迅 的One-Hot编码为: [0, 1, 0, 0, 0, 0]\n",
      "王力宏 的One-Hot编码为: [0, 0, 1, 0, 0, 0]\n",
      "李宗盛 的One-Hot编码为: [0, 0, 0, 1, 0, 0]\n",
      "吴亦凡 的One-Hot编码为: [0, 0, 0, 0, 1, 0]\n",
      "鹿晗 的One-Hot编码为: [0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['./Tokenizer']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab = (\"周杰伦\", \"陈奕迅\", \"王力宏\", \"李宗盛\", \"吴亦凡\", \"鹿晗\")\n",
    "t = Tokenizer(num_words=None, char_level=False)\n",
    "t.fit_on_texts(vocab)\n",
    "\n",
    "for token in vocab:\n",
    "    zero_list = [0] * len(vocab)\n",
    "    token_index = t.texts_to_sequences([token])[0][0] - 1\n",
    "    zero_list[token_index] = 1\n",
    "    print(token, \"的One-Hot编码为:\", zero_list)\n",
    "\n",
    "tokenizer_path = \"./Tokenizer\"\n",
    "joblib.dump(t, tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李宗盛 的One-Hot编码为: [0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "t = joblib.load(tokenizer_path)\n",
    "\n",
    "token = \"李宗盛\"\n",
    "token_index = t.texts_to_sequences([token])[0][0] - 1\n",
    "zero_list = [0] * len(vocab)\n",
    "zero_list[token_index] = 1\n",
    "print(token, \"的One-Hot编码为:\", zero_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优势：操作简单，容易理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "劣势：完全割裂了词与词之间的关联，而且在大语料集下，每个向量的长度过大，占据大量内存，所以现在用这种办法很少了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Word2Vec\n",
    "这是一种流行的将词汇表示成词向量的无监督训练方法，该过程将构建神经网络模型，将网络参数作为词汇的词向量表示，它包含CBOW和skipgram两种训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.CBOW(Continuous bag of words，连续词袋模式)模式\n",
    "给定一段用于训练的文本语料，在选定某段长度(窗口)作为研究对象，使用上下词汇预测目标词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中(the quick brown fox jumps over the lazy dog)窗口大小为9，使用前后4个词汇对目标词汇进行预测。相当于我知道了上文的四个单词，也知道了下文的四个单词，来预测中间的Jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们给定的训练预料只有一句话:Hope can set you free，窗口大小为3，因此模型的第一个训练样本来自Hope can set，因为是CBOW模式，所以将使用Hope和Set作为输入，Can作为输出，在模型训练时，Hope、can、set等词汇都是用他们的one-hot编码，如图所示：每个one-hot编码的单词与各自的变换矩阵(即参数矩阵$3 \\times 5$这里的3指的是得到的词向量维度)相城之后再相加，得到上下文表示矩阵$3 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，将上下文表示矩阵与变换矩阵(参数矩阵$5 \\times 3$，所有的变换矩阵共享参数)相乘，得到$5 \\times 1$进行损失的计算，然后更新网络参数完成一次模型迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后窗口按序向后移动，重新更新参数，知道所有语料都被遍历完成，得到最终的变换矩阵$3 \\times 5$，这个变换矩阵与每个词汇的one-hot编码$5 \\times 1$相乘，得到的$3 \\times 1$的矩阵就是该词汇的word2vec张量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.Skip-Gram过程\n",
    "Skip-Gram与CBOW的过程正好相反，它是给定中间词，去预测上下文的别的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们给定的训练语料只有一句话:Hope can set you free，窗口大小为3，因此模型的第一个训练样本来自Hope can set，那么输入就是can，而输出就是hope与set。在模型训练时，Hope，can，set等词汇都是用它们的one-hot编码，如图所示，将can的one-hot编码与变换矩阵(即参数矩阵$3 \\times 5$，这里的3是指最后得到的词向量的维度)相乘，得到目标词汇表示矩阵$3 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，将目标词汇表示矩阵与多个变换矩阵$5 \\times 3$相乘，得到多个$5 \\times 1$的结果矩阵，它将与我们hope和set对应的one-hot编码矩阵进行损失的计算，然后更新网络参数完成一次模型迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后窗口按序向后移动，重新更新参数，知道所有语料遍历完成，得到最终的变换矩阵即参数矩阵$3 \\times 5$，这个变换矩阵与每个词汇的one-hot编码相乘，得到的$3 \\times 1$的矩阵就是该词汇的word2vec张量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.代码实现\n",
    "- 获取训练数据\n",
    "- 训练词向量\n",
    "- 模型超参数设定\n",
    "- 模型效果检验\n",
    "- 模型的保存与重加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"/Users/JJZHK/data/input/enwiki/fil9_skip.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.3053602 , -0.03778189,  0.18975118, -0.3609041 , -0.02635356,\n        0.1533042 , -0.32088202,  0.02182054, -0.2195464 , -0.11102954,\n        0.00680499,  0.34333256,  0.2614339 , -0.17481951,  0.24838704,\n        0.10710759, -0.12162277, -0.03347991,  0.2812598 , -0.20552237,\n        0.02938094,  0.02305067,  0.07528982, -0.13916977, -0.15943722,\n       -0.02781923, -0.14546995,  0.08239675,  0.15990876, -0.04541094,\n        0.18323246, -0.1194833 ,  0.08541378,  0.03854949,  0.11612419,\n        0.13029115, -0.09372667, -0.06965848,  0.00327052,  0.3930265 ,\n        0.07005941,  0.26403233,  0.15834227,  0.07445641, -0.00198202,\n       -0.50928795, -0.07657424, -0.17984582, -0.05191897, -0.16971368,\n        0.01971304,  0.01513219,  0.25546676, -0.04594915,  0.01755979,\n       -0.0892631 , -0.1350921 , -0.28731844,  0.03261127,  0.21875954,\n       -0.12447338,  0.24096492,  0.35495415,  0.17329416,  0.02497513,\n        0.15604764, -0.0074317 , -0.16222757,  0.02680165, -0.19226155,\n       -0.0262446 , -0.07975991,  0.22985983, -0.19305249, -0.05719522,\n        0.3014344 , -0.2853542 ,  0.06081726,  0.18318422, -0.11017192,\n        0.04835566,  0.16455793, -0.19388197, -0.21114871,  0.13007936,\n        0.01819335,  0.1335004 , -0.2724739 , -0.3250484 , -0.04944852,\n        0.05644027,  0.1009498 ,  0.14072601,  0.11535967, -0.11110705,\n       -0.41597798, -0.00948166, -0.2667743 ,  0.29504853, -0.03924265],\n      dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.8656336665153503, 'sport'),\n (0.8596043586730957, 'sporting'),\n (0.8225077390670776, 'sportsnet'),\n (0.8216253519058228, 'sportsground'),\n (0.8146790862083435, 'sportsplex'),\n (0.8103185892105103, 'sportscar'),\n (0.8072801828384399, 'motorsports'),\n (0.802819013595581, 'sportscars'),\n (0.792915403842926, 'athletics'),\n (0.7810478806495667, 'sportsplane')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.850132942199707, 'dogs'),\n (0.8089784383773804, 'hound'),\n (0.8058704137802124, 'sheepdogs'),\n (0.7966912388801575, 'sheepdog'),\n (0.7813048958778381, 'deerhound'),\n (0.7605849504470825, 'puppies'),\n (0.7555603384971619, 'hounds'),\n (0.7541096210479736, 'coonhounds'),\n (0.7526222467422485, 'cat'),\n (0.7508794069290161, 'coonhound')]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Word Embedding\n",
    "- 通过一定的方式将词汇映射到指定维度(一般是更高维度)的空间\n",
    "- 广义的word Embedding包括所有密集词汇向量的表示方法，如之前学习的word2vec，即可认为是word embedding的一种\n",
    "- 狭义的word embedding是指在神经网络中加入embedding层，对整个网络进行训练的同时产生的embedding矩阵(embedding层的参数)，这个embedding矩阵就是训练过程中所有输入词汇的向量表示组成的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import fileinput\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "embedded = torch.randn(100, 50)\n",
    "meta = list(map(lambda x : x.strip(), fileinput.FileInput(\"/Users/JJZHK/data/input/enwiki/vocab100.csv\")))\n",
    "writer.add_embedding(embedded, metadata=meta)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}