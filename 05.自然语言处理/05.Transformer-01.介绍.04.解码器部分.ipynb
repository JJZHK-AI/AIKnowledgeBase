{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-解码器部分\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由N个解码器层堆叠而成\n",
    "- 每个解码器层由三个子层连接结构组成\n",
    "- 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接\n",
    "- 第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接\n",
    "- 第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.解码器层\n",
    "作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-27.7572,  29.6178,  33.7259,  ...,   0.0000,  -4.9358,  23.9576],\n         [  0.0000, -47.3996,  -8.3996,  ...,   0.0000,  28.9097,  42.4398],\n         [ 39.8116, -13.7847,   0.0000,  ...,  -0.0000,   0.0000, -21.2629],\n         [ -0.0000, -28.5188,  25.8290,  ..., -14.9359, -17.5258,  18.3313]],\n\n        [[  0.0000,   5.1443,   4.8328,  ...,  51.9236,  -4.9627,   0.0000],\n         [ -2.8123,   5.7593,  29.0198,  ...,  28.0604,   0.0000,  24.1394],\n         [-16.3330,  -0.0000, -30.9909,  ...,   1.3160,  17.7589,  -0.0000],\n         [-15.2141,  -0.0000,   2.5617,  ...,   0.0000,  -0.6455,  22.9396]]],\n       grad_fn=<MulBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.PositionalEncoding import PositionalEncoding\n",
    "from lib.Embeddings import Embeddings\n",
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "dropout = 0.2\n",
    "max_len=60\n",
    "vocab = 1000\n",
    "\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "emb = Embeddings(d_model, vocab)\n",
    "# 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4\n",
    "x = torch.autograd.Variable(torch.LongTensor([[100,2,421,508],[491,998,1,221]]))\n",
    "embr = emb(x)\n",
    "# 输入x是Embedding层的输出的张量, 形状是2 x 4 x 512\n",
    "x = embr\n",
    "pe_result = pe(x)\n",
    "x = pe_result\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-27.9119,  29.4952,  34.2609,  ...,   1.0484,  -4.9317,  23.4846],\n",
      "         [  0.3368, -47.0936,  -8.6950,  ...,   0.6991,  28.6446,  42.1238],\n",
      "         [ 40.0375, -13.9946,   0.5067,  ...,   0.8552,   0.1961, -21.8864],\n",
      "         [  1.0073, -28.5008,  25.9462,  ..., -14.4552, -17.4981,  18.0162]],\n",
      "\n",
      "        [[  0.8172,   4.6504,   5.5159,  ...,  51.9619,  -5.1755,   0.2245],\n",
      "         [ -2.2191,   5.2802,  30.0046,  ...,  28.0927,  -0.3330,  24.4751],\n",
      "         [-15.7612,  -0.9180, -29.8939,  ...,   1.9881,  17.6986,   0.5592],\n",
      "         [-14.5487,  -0.1786,   3.3623,  ...,  -0.4087,  -0.3726,  23.0910]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "from lib.PositionwiseFeedForward import PositionwiseFeedForward\n",
    "import copy\n",
    "from lib.EncoderLayer import EncoderLayer\n",
    "from lib.Encoder import Encoder\n",
    "from lib.DecoderLayer import DecoderLayer\n",
    "\n",
    "# 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.\n",
    "head = 8\n",
    "size = 512\n",
    "d_ff = 64\n",
    "\n",
    "self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)\n",
    "\n",
    "# 前馈全连接层也和之前相同 \n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "# x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.\n",
    "\n",
    "# memory是来自编码器的输出\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "# 编码器中编码器层的个数N\n",
    "N = 8\n",
    "en = Encoder(layer, N)\n",
    "en_result = en(x, mask)\n",
    "memory = en_result\n",
    "\n",
    "# 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask\n",
    "source_mask = target_mask = mask\n",
    "dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)\n",
    "dl_result = dl(x, memory, source_mask, target_mask)\n",
    "print(dl_result)\n",
    "print(dl_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.解码器\n",
    "根据编码器的结果以及上一次预测的结果, 对下一次可能出现的'值'进行特征表示."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8165,  1.2935,  1.1358,  ..., -0.1313, -0.0205,  0.8622],\n",
      "         [ 0.1687, -1.7185, -0.4857,  ..., -0.1639,  1.3899,  1.6002],\n",
      "         [ 2.0399, -0.3050,  0.1206,  ..., -0.2707,  0.0828, -0.8054],\n",
      "         [ 0.2085, -0.9998,  0.9480,  ..., -0.8538, -0.5872,  0.6681]],\n",
      "\n",
      "        [[ 0.2624,  0.2000,  0.0410,  ...,  2.1882, -0.2590, -0.0736],\n",
      "         [ 0.0030,  0.3192,  1.0609,  ...,  1.0737, -0.1478,  0.8669],\n",
      "         [-0.4218,  0.0692, -1.2368,  ...,  0.0297,  0.5732,  0.1169],\n",
      "         [-0.3526,  0.2811,  0.1286,  ...,  0.1508, -0.0563,  1.0132]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from lib.MultiHeadedAttention import MultiHeadedAttention\n",
    "from lib.PositionwiseFeedForward import PositionwiseFeedForward\n",
    "from lib.DecoderLayer import DecoderLayer\n",
    "from lib.Decoder import Decoder\n",
    "# 分别是解码器层layer和解码器层的个数N\n",
    "size = 512\n",
    "d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
    "N = 8\n",
    "\n",
    "# 输入参数与解码器层的输入参数相同\n",
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = torch.autograd.Variable(torch.zeros(8, 4, 4))\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "de = Decoder(layer, N)\n",
    "de_result = de(x, memory, source_mask, target_mask)\n",
    "print(de_result)\n",
    "print(de_result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0962ff1a08bbd29f414ba67199d725d5b08f35603471a3d6cc67d6569664ed27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}