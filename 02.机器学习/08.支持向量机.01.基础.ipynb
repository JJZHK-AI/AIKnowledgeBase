{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "支持向量机\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.问题引入\n",
    "![images](images/08_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中的(a)是已有的数据，红色和蓝色分别代表两个不同的类别。数据显然是线性可分的，但是将两类数据点分开的直线显\n",
    "然不止一条。上图的(b)和(c)分别给出了B、C两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策\n",
    "面对应了一个线性分类器。虽然从分类结果上看，分类器A和分类器B的效果是相同的。但是他们的性能是有差距的，看下\n",
    "图："
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![images](images/08_002.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数学建模"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置。每幅图中的两条虚线中间的距离，我们叫做$D$，使得这个$D$最大的两个虚面，我们叫做\"支撑平面\"，图中的实线，叫做分割面。在支撑平面上的样本点，叫做支撑向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数学建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1.空间中的点、线、面\n",
    "假设空间中存在一个点$(x_0,y_0)$，以及一条直线$f(x,y)=Ax+By+C$，那么我们知道这个点到这条直线的距离就是\n",
    "$$D=\\frac{|Ax_0+By_0+C|}{\\sqrt{A^2+B^2}}$$\n",
    "如果$Ax_0+By_0+C > 0$，说明$(x_0,y_0)$位于法向$(A,B)$的正向，反之位于负向，扩展到多维向量，点到直线的距离是\n",
    "$$\\alpha(x_0,L)=\\frac{\\overrightarrow{\\omega} \\bullet \\overrightarrow{x} + C}{||\\omega||}$$\n",
    "那么对于平面来说呢，我们知道空间中的平面方程为\n",
    "$$\\omega^Tx+b=0$$\n",
    "其中$\\omega$是平面的法向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2.分割面与支撑平面方程\n",
    "对于二分类问题，我们对每个样本点$x_i$加上一个类别标签$y_i$,则有\n",
    "$$\n",
    "y_i=\\begin{cases}\n",
    "+1 & red \\\\\n",
    "-1 & blue\n",
    "\\end{cases}\n",
    "$$\n",
    "对于分割面来说，它的方程就是\n",
    "$$\\omega^Tx+b=0$$\n",
    "对于两个分割面来说，它们的方程就是\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\omega^Tx+b>=1 & \\forall y_i = 1\\\\\\\\\n",
    "\\omega^Tx+b<=-1 & \\forall y_i =-1\n",
    "\\end{cases}\n",
    "$$\n",
    "由于我们假定$|y_i|=1$，那么就有$y_i(\\omega^Tx_i+b)>=1,\\forall x_i$，等于1的时候，那就是支撑平面上的点，我们要计算的是支撑平面距离最大的情况，支撑平面的距离就是$D=\\frac{2}{||\\omega||}$,也就是我们的目标函数是\n",
    "$$argmax(D)=argmax(\\frac{2}{||\\omega||})=argmin(\\frac{||\\omega||}{2})$$\n",
    "我们知道，$\\omega=\\sqrt{\\sum_{i=1}^mx_i^2}$，那么其实上述问题的目标函数可以变为\n",
    "$$argmin(\\frac{||\\omega||^2}{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3.问题给出\n",
    "- 假设给定一个特征空间上的训练数据集$T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$，其中$x_i \\in R^n, y_i \\in \\{-1,+1\\},i=1,2,...,N$\n",
    "- $x_i$为第i个实例(若n>1,$x_i$为向量)\n",
    "- $y_i$为$x_i$的类标记；当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例\n",
    "- $(x_i,y_i)$称为样本点\n",
    "- 要把所有样本都分开，其次你要在所有这些可以把样本都分开的线里面找到一个距离样本最远的直线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4.目标与约束\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "& &min\\frac{1}{2}||\\omega||^2 \\\\\n",
    "s.t.& &y_i(\\omega^Tx_i+b) \\geq 1,i=1,2,...,n\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "我们一看这就是个凸优化问题，那么必然会用到拉格朗日乘子法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.5.问题转化\n",
    "约束可以转化为\n",
    "$$1-y(\\omega^Tx+b)\\leq 0$$\n",
    "加入拉格朗日因子$\\alpha$,变化为\n",
    "$$-\\alpha(y(w^Tx+b)-1)\\leq 0$$\n",
    "那么拉格朗日函数就是\n",
    "$$L(\\omega,b,\\alpha)=\\frac{1}{2}||\\omega||^2-\\alpha(y(\\omega^Tx+b)-1)=\\frac{1}{2}||\\omega||^2-\\sum_{i=1}^n\\alpha_i(y_i(\\omega^Tx_i+b)-1)$$\n",
    "其中$x_i$就是第i个样本,$y_i$就是第i个样本对应的分类,$\\alpha_i$就是第i个样本对应的拉格朗日乘子,$\\alpha_i\\geq 0$，那么我们我们要求的是一个极小极大问题\n",
    "$$min_{\\omega,b}\\theta(\\omega)=min_{\\omega,b}max_{\\alpha_i\\geq 0}L(\\omega,b,\\alpha)=p^*$$\n",
    "然后我们找到此问题的拉格朗日对偶函数\n",
    "$$max_{\\alpha_i\\geq 0}min_{\\omega,b}L(\\omega,b,\\alpha)=d^*$$\n",
    "我们知道$d^* \\leq p^*$，我们关心的是$d^*=p^*$的情况，那么什么情况下等式成立呢，我们知道那就是要用到的KKT条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6.问题符合KKT条件\n",
    "由于有\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "min_{\\omega,b}max_{\\alpha_i}L(\\omega,b,\\alpha)&=&min\\frac{1}{2}||\\omega||^2\\\\\n",
    "L(\\omega,b,\\alpha)&=&\\frac{1}{2}||\\omega||^2-\\sum_{i=1}^n\\alpha_i(y_i(\\omega^Tx_i+b)-1),\\alpha_i\\geq 0,y_i(\\omega^Tx_i+b)\\geq 1\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "因此只有每一个$\\alpha_i(y_i(\\omega^Tx_i+b)-1)$都是0，结果才能为0从而达到才能取得最大值。这样我们就能得到KKT条件:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\alpha_i\\geq 0 \\\\\\\\\n",
    "y_i(\\omega^Tx_i+b)-1\\geq 0 \\\\\\\\\n",
    "\\alpha_i(y_i(\\omega^Tx_i+b)-1)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "根据KKT条件3，我们可以得出这样一个结论$\\alpha_i=0$或者$y_i(\\omega^Tx_i+b)=1$.所以如果一个样本是支持向量，则其对应的拉格朗日系数非零；如果一个样本不是支持向量，则其对应的拉格朗日系数一定为0。也就是说我们要找的支撑平面只跟支撑向量有关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.7.对偶问题求解\n",
    "对于$L(\\omega,b,\\alpha)$,可以对$\\omega，b$求偏导数，令其结果为0，消掉$\\omega,b$然后在对$\\alpha$求L的最大值。有：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{L}}{\\partial{\\omega}}&=0\\\\\n",
    "&\\Rightarrow& \\frac{1}{2}*2||\\omega||-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\\\\n",
    "&\\Rightarrow& \\omega=\\sum_{i=1}^n\\alpha_iy_ix_i\\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}}&=&0\\\\\n",
    "&\\Rightarrow& \\sum_{i=1}^n\\alpha_iy_i=0\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这是两个非常重要的等式，然后我们展开原来的式子:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "L(\\omega,b,\\alpha)&=&\\frac{1}{2}||\\omega||^2-\\sum_{i=1}^n\\alpha_i(y_i(\\omega^Tx_i+b)-1)\\\\\n",
    "&=&\\frac{1}{2}\\omega^T \\bullet \\omega-\\sum_{i=1}^n\\alpha_iy_i\\omega^Tx_i-\\sum_{i=1}^n\\alpha_iy_ib+\\sum_{i=1}^n\\alpha_i\\\\\n",
    "&=&\\frac{1}{2}\\omega^T \\bullet \\omega-\\omega^T\\sum_{i=1}^n\\alpha_iy_ix_i-b\\sum_{i=1}^n\\alpha_iy_i+\\sum_{i=1}^n\\alpha_i\\\\\n",
    "&\\Rightarrow& \\frac{1}{2}\\omega^T\\sum_{i=1}^n\\alpha_iy_ix_i-\\omega^T\\sum_{i=1}^n\\alpha_iy_ix_i-b \\bullet 0+\\sum_{i=1}^n\\alpha_i\\\\\n",
    "&=&\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\omega^T\\sum_{i=1}^n\\alpha_iy_ix_i\\\\\\ \\\n",
    "&\\Rightarrow& \\sum_{i=1}^n\\alpha_i-\\frac{1}{2}[\\sum_{i=1}^n\\alpha_iy_ix_i]^T\\sum_{i=1}^n\\alpha_iy_ix_i\\\\\\ \\\n",
    "&=&\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "其中$\\alpha_i\\geq 0$,于是我们把\n",
    "$$max_{\\alpha_i\\geq 0}min_{\\omega,b}L(\\omega,b,\\alpha)=d^*$$\n",
    "变为了\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "& &max_{\\alpha_i}[\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j]\\\\\n",
    "s.t. & &\\alpha_i \\geq 0, i=1,2,...,n\\\\\\ \\\n",
    "& &\\sum_{i=1}^n\\alpha_iy_i=0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "可以变为\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "& &min_{\\alpha_i}[\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i]\\\\\\ \\\n",
    "s.t. & &\\alpha_i \\geq 0, i=1,2,...,n\\\\\\ \\\n",
    "& &\\sum_{i=1}^n\\alpha_iy_i=0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "实际上，对于上述目标函数，是存在一个假设的，即数据100%线性可分。但是，目前为止，我们知道几乎所有数据都不那么\"干净\"。这时我们就可以通过引入所谓的松弛变量(slack variable)，来允许有些数据点可以处于超平面的错误的一侧。所以就变为了\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "& &min_{\\alpha_i}[\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{i=1}^n\\alpha_i]\\\\\\ \\\n",
    "s.t. & &0 \\leq \\alpha_i \\leq C, i=1,2,...,n\\\\\\ \\\n",
    "& &\\sum_{i=1}^n\\alpha_iy_i=0\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ]
}