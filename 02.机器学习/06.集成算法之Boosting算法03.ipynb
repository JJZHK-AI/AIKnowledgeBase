{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Boosting算法系列\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.AdaBoost实例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.1.使用AdaBoost对鸢尾花进行分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"data\")\n",
    "# 花萼长度、花萼宽度，花瓣长度，花瓣宽度\n",
    "iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'\n",
    "iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'\n",
    "iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'iris.data'), header=None)\n",
    "x = data[np.arange(4)]\n",
    "y = pd.Categorical(data[4]).codes\n",
    "\n",
    "x = x.iloc[:, :2]# 为了可视化，仅使用前两列特征\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# 决策树参数估计\n",
    "base_estimator=[LogisticRegression()]\n",
    "max_depth = np.arange(1, 10)\n",
    "algorithm=['SAMME','SAMME.R']\n",
    "learning_rate = [0.1, 0.5, 1]\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "params_list = dict(learning_rate=learning_rate, n_estimators = n_estimators)\n",
    "cv = StratifiedKFold(4)\n",
    "cv.get_n_splits(x_train, y_train)\n",
    "\n",
    "model = GridSearchCV(AdaBoostClassifier(), param_grid=params_list, cv=cv)\n",
    "model.fit(x_train, y_train)\n",
    "y_test_hat = model.predict(x_test)      # 测试数据\n",
    "print(model.best_params_)\n",
    "print('Score:', model.score(x_test, y_test))\n",
    "\n",
    "# 画图\n",
    "N, M = 50,50 # 横纵各采样多少个值\n",
    "x1_min, x2_min = x.min()\n",
    "x1_max, x2_max = x.max()\n",
    "t1 = np.linspace(x1_min, x1_max, N)\n",
    "t2 = np.linspace(x2_min, x2_max, M)\n",
    "x1, x2 = np.meshgrid(t1, t2)  # 生成网格采样点\n",
    "x_show = np.stack((x1.flat, x2.flat), axis=1)  # 测试点\n",
    "\n",
    "cm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])\n",
    "cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])\n",
    "y_show_hat = model.predict(x_show)  # 预测值\n",
    "y_show_hat = y_show_hat.reshape(x1.shape)  # 使之与输入的形状相同\n",
    "plt.figure(figsize=(10,10),facecolor='w')\n",
    "plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light)  # 预测值的显示\n",
    "plt.scatter(x_test[0], x_test[1], c=y_test.ravel(), edgecolors='k', s=150, zorder=10, cmap=cm_dark, marker='*')  # 测试数据\n",
    "plt.scatter(x[0], x[1], c=y.ravel(), edgecolors='k', s=40, cmap=cm_dark)  # 全部数据\n",
    "plt.xlabel(iris_feature[0], fontsize=15)\n",
    "plt.ylabel(iris_feature[1], fontsize=15)\n",
    "plt.xlim(x1_min, x1_max)\n",
    "plt.ylim(x2_min, x2_max)\n",
    "plt.grid(True)\n",
    "plt.title(u'鸢尾花数据的Adaboost分类', fontsize=17)\n",
    "\n",
    "# 训练集上的预测结果\n",
    "y_test = y_test.reshape(-1)\n",
    "result = (y_test_hat == y_test)   # True则预测正确，False则预测错误\n",
    "acc = np.mean(result)\n",
    "print('准确度: %.2f%%' % (100 * acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.2.AdaBoost进行曲线回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(linewidth=1000)\n",
    "N = 9\n",
    "x = np.linspace(0, 8, N) + np.random.randn(N)\n",
    "x = np.sort(x)\n",
    "y = x**2 - 4*x - 3 + np.random.randn(N)\n",
    "x.shape = -1, 1\n",
    "y.shape = -1, 1\n",
    "\n",
    "deep_length = 1\n",
    "model = AdaBoostRegressor(n_estimators=100)\n",
    "clrs = []  # 颜色\n",
    "for c in np.linspace(16711680, 255, deep_length):\n",
    "    clrs.append(\"#{:06X}\".format(int(c), 2))\n",
    "d_pool = np.arange(1, deep_length + 1, 1)  # 阶\n",
    "\n",
    "label = 'Adaboost回归'\n",
    "\n",
    "plt.figure(figsize=(18, 12), facecolor='w')\n",
    "plt.plot(x, y, 'ro', ms=10, zorder=N)\n",
    "\n",
    "for i, d in enumerate(d_pool):\n",
    "    #model.set_params(max_depth=d)\n",
    "    model.fit(x, y.ravel())\n",
    "\n",
    "    x_hat = np.linspace(x.min(), x.max(), num=10)\n",
    "    x_hat.shape = -1, 1\n",
    "    y_hat = model.predict(x_hat)\n",
    "    s = model.score(x, y)\n",
    "    mse = np.average((y_hat - np.array(y)) ** 2)\n",
    "    label1 = u'$R^2$=%.3f, MSE=%.3f' % (s, mse)\n",
    "    plt.plot(x_hat, y_hat, color=clrs[i], lw=3, alpha=0.75, label=label1)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.title(label, fontsize=18)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "\n",
    "plt.tight_layout(1, rect=(0, 0, 1, 0.95))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8.梯度提升树-GBDT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1.对鸢尾花数据进行分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"data\")\n",
    "# 花萼长度、花萼宽度，花瓣长度，花瓣宽度\n",
    "iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'\n",
    "iris_feature = u'花萼长度', u'花萼宽度', u'花瓣长度', u'花瓣宽度'\n",
    "iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'iris.data'), header=None)\n",
    "x = data[np.arange(4)]\n",
    "y = pd.Categorical(data[4]).codes\n",
    "\n",
    "x = x.iloc[:, :2]# 为了可视化，仅使用前两列特征\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# 决策树参数估计\n",
    "max_depth = np.arange(1, 10)\n",
    "learning_rate = [0.1, 0.5, 1]\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "params_list = dict(max_depth = max_depth, learning_rate=learning_rate, n_estimators = n_estimators)\n",
    "cv = StratifiedKFold(4)\n",
    "cv.get_n_splits(x_train,y_train)\n",
    "\n",
    "model = GridSearchCV(GradientBoostingClassifier(random_state=0), param_grid=params_list, cv=cv)\n",
    "#使用信息熵来划分子树，gini：使用gini系数划分子树\n",
    "model.fit(x_train, y_train)\n",
    "y_test_hat = model.predict(x_test)      # 测试数据\n",
    "print(model.best_params_)\n",
    "print('得分:%.2f' % (100 * model.score(x_test, y_test)))\n",
    "# 画图\n",
    "N, M = 50,50 # 横纵各采样多少个值\n",
    "x1_min, x2_min = x.min()\n",
    "x1_max, x2_max = x.max()\n",
    "t1 = np.linspace(x1_min, x1_max, N)\n",
    "t2 = np.linspace(x2_min, x2_max, M)\n",
    "x1, x2 = np.meshgrid(t1, t2)  # 生成网格采样点\n",
    "x_show = np.stack((x1.flat, x2.flat), axis=1)  # 测试点\n",
    "\n",
    "cm_light = mpl.colors.ListedColormap(['#A0FFA0', '#FFA0A0', '#A0A0FF'])\n",
    "cm_dark = mpl.colors.ListedColormap(['g', 'r', 'b'])\n",
    "y_show_hat = model.predict(x_show)  # 预测值\n",
    "y_show_hat = y_show_hat.reshape(x1.shape)  # 使之与输入的形状相同\n",
    "plt.figure(figsize=(20,10), facecolor='w')\n",
    "plt.pcolormesh(x1, x2, y_show_hat, cmap=cm_light)  # 预测值的显示\n",
    "plt.scatter(x_test[0], x_test[1], c=y_test.ravel(), edgecolors='k', s=150, zorder=10, cmap=cm_dark, marker='*')  # 测试数据\n",
    "plt.scatter(x[0], x[1], c=y.ravel(), edgecolors='k', s=40, cmap=cm_dark)  # 全部数据\n",
    "plt.xlabel(iris_feature[0], fontsize=15)\n",
    "plt.ylabel(iris_feature[1], fontsize=15)\n",
    "plt.xlim(x1_min, x1_max)\n",
    "plt.ylim(x2_min, x2_max)\n",
    "plt.grid(True)\n",
    "plt.title(u'鸢尾花数据的GDBT分类', fontsize=17)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练集上的预测结果\n",
    "y_test = y_test.reshape(-1)\n",
    "result = (y_test_hat == y_test)   # True则预测正确，False则预测错误\n",
    "acc = np.mean(result)\n",
    "print('准确度: %.2f%%' % (100 * acc))\n",
    "\n",
    "# 过拟合：错误率\n",
    "depth = np.arange(1, 15)\n",
    "err_list = []\n",
    "for d in depth:\n",
    "    clf = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=d, random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_test_hat = clf.predict(x_test)  # 测试数据\n",
    "    result = (y_test_hat == y_test)  # True则预测正确，False则预测错误\n",
    "\n",
    "    err = 1 - np.mean(result)\n",
    "    err_list.append(err)\n",
    "    print(d, ' 错误率: %.2f%%' % (100 * err))\n",
    "plt.plot(depth, err_list, 'ro-', lw=2)\n",
    "plt.xlabel(u'决策树深度', fontsize=15)\n",
    "plt.ylabel(u'错误率', fontsize=15)\n",
    "plt.title(u'决策树深度与过拟合', fontsize=17)\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2.进行曲线回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(linewidth=1000)\n",
    "N = 9\n",
    "x = np.linspace(0, 8, N) + np.random.randn(N)\n",
    "x = np.sort(x)\n",
    "y = x**2 - 4*x - 3 + np.random.randn(N)\n",
    "x.shape = -1, 1\n",
    "y.shape = -1, 1\n",
    "\n",
    "deep_length = 1\n",
    "model = GradientBoostingRegressor(n_estimators=100)\n",
    "clrs = []  # 颜色\n",
    "for c in np.linspace(16711680, 255, deep_length):\n",
    "    clrs.append(\"#{:06X}\".format(int(c), 2))\n",
    "d_pool = np.arange(1, deep_length + 1, 1)  # 阶\n",
    "\n",
    "label = 'GBRT回归'\n",
    "\n",
    "plt.figure(figsize=(18, 12), facecolor='w')\n",
    "plt.plot(x, y, 'ro', ms=10, zorder=N)\n",
    "\n",
    "for i, d in enumerate(d_pool):\n",
    "    model.set_params(max_depth=d)\n",
    "    model.fit(x, y.ravel())\n",
    "\n",
    "    x_hat = np.linspace(x.min(), x.max(), num=10)\n",
    "    x_hat.shape = -1, 1\n",
    "    y_hat = model.predict(x_hat)\n",
    "    s = model.score(x, y)\n",
    "    mse = np.average((y_hat - np.array(y)) ** 2)\n",
    "    label1 = u'决策树深度:%d，$R^2$=%.3f, MSE=%.3f' % (d, s, mse)\n",
    "    plt.plot(x_hat, y_hat, color=clrs[i], lw=3, alpha=0.75, label=label1)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.title(label, fontsize=18)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "plt.tight_layout(1, rect=(0, 0, 1, 0.95))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9.使用Python Code实现梯度提升树"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class DecisionNode():\n",
    "    def __init__(self, feature_i=None, threshold=None,value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  #根节点\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        # 计算值 如果是分类问题就是信息增益，回归问题就基尼指数\n",
    "        self._impurity_calculation = None\n",
    "        self._leaf_value_calculation = None #计算叶子\n",
    "        self.one_dim = None\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\"\n",
    "        递归求解树\n",
    "        \"\"\"\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None\n",
    "        best_sets = None\n",
    "\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # 计算每一个特征的增益值\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                for threshold in unique_values:\n",
    "                    Xy1, Xy2 = self.divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # 计算增益值\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],\n",
    "                                \"lefty\": Xy1[:, n_features:],\n",
    "                                \"rightX\": Xy2[:, :n_features],\n",
    "                                \"righty\": Xy2[:, n_features:]\n",
    "                            }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # 计算节点的目标值\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\"\n",
    "        预测\n",
    "        \"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def divide_on_feature(self, X, feature_i, threshold):\n",
    "        split_func = None\n",
    "        if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "            split_func = lambda sample: sample[feature_i] >= threshold\n",
    "        else:\n",
    "            split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "        X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "        X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "        return np.array([X_1, X_2])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.predict_value(x))\n",
    "        return y_pred\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    log2 = math.log2\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.1.分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def calculate_variance(X):\n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "    n_samples = np.shape(X)[0]\n",
    "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
    "\n",
    "    return variance\n",
    "\n",
    "class RegressionTree(DecisionTree):\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "\n",
    "        # 使用方差缩减\n",
    "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "\n",
    "        return sum(variance_reduction)\n",
    "\n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)\n",
    "\n",
    "class GradientBoosting(object):\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_impurity, max_depth, regression):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.regression = regression\n",
    "\n",
    "        self.loss = CrossEntropy()\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = RegressionTree(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_impurity=min_impurity,\n",
    "                max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "        for i in range(self.n_estimators):\n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            update = self.trees[i].predict(X)\n",
    "            # Update y prediction\n",
    "            y_pred -= np.multiply(self.learning_rate, update)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            update = np.multiply(self.learning_rate, update)\n",
    "            y_pred = -update if not y_pred.any() else y_pred - update\n",
    "\n",
    "        if not self.regression:\n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)\n",
    "\n",
    "\n",
    "\n",
    "class GradientBoostingClassifier(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
    "        super(GradientBoostingClassifier, self).__init__(n_estimators=n_estimators,\n",
    "                                                         learning_rate=learning_rate,\n",
    "                                                         min_samples_split=min_samples_split,\n",
    "                                                         min_impurity=min_info_gain,\n",
    "                                                         max_depth=max_depth,\n",
    "                                                         regression=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = to_categorical(y)\n",
    "        super(GradientBoostingClassifier, self).fit(X, y)\n",
    "def to_categorical(x, n_col=None):\n",
    "    \"\"\" One-hot encoding of nominal values \"\"\"\n",
    "    if not n_col:\n",
    "        n_col = np.amax(x) + 1\n",
    "    one_hot = np.zeros((x.shape[0], n_col))\n",
    "    one_hot[np.arange(x.shape[0]), x] = 1\n",
    "    return one_hot\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.2.回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(\"data\")\n",
    "mpl.rcParams['font.sans-serif'] = [u'simHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def calculate_variance(X):\n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "    n_samples = np.shape(X)[0]\n",
    "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
    "\n",
    "    return variance\n",
    "class RegressionTree(DecisionTree):\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "\n",
    "        # 使用方差缩减\n",
    "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "\n",
    "        return sum(variance_reduction)\n",
    "\n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)\n",
    "\n",
    "class GradientBoosting(object):\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_impurity, max_depth, regression):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.regression = regression\n",
    "\n",
    "        self.loss = SquareLoss()\n",
    "\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = RegressionTree(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_impurity=min_impurity,\n",
    "                max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "        for i in range(self.n_estimators):\n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            update = self.trees[i].predict(X)\n",
    "            # Update y prediction\n",
    "            y_pred -= np.multiply(self.learning_rate, update)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            update = np.multiply(self.learning_rate, update)\n",
    "            y_pred = -update if not y_pred.any() else y_pred - update\n",
    "\n",
    "        if not self.regression:\n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class SquareLoss(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "class GradientBoostingRegressor(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
    "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
    "        super(GradientBoostingRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                                        learning_rate=learning_rate,\n",
    "                                                        min_samples_split=min_samples_split,\n",
    "                                                        min_impurity=min_var_red,\n",
    "                                                        max_depth=max_depth,\n",
    "                                                        regression=True)\n",
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'MyData.txt'), sep=\"\\t\")\n",
    "time = np.atleast_2d(data[\"time\"].values).T\n",
    "temp = np.atleast_2d(data[\"temp\"].values).T\n",
    "\n",
    "X = time.reshape((-1, 1))               # Time. Fraction of the year [0, 1]\n",
    "X = np.insert(X, 0, values=1, axis=1)   # Insert bias term\n",
    "y = temp[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_line = model.predict(X)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "\n",
    "m1 = plt.scatter(366 * X_train[:, 1],y_train, c='r',s=10, label=u'训练集')\n",
    "m2 = plt.scatter(366 * X_test[:, 1], y_test,c='g',s=10, label=u'测试集')\n",
    "m3 = plt.scatter(366 * X_test[:, 1], y_pred,c='b',s=10, label=u'预测集')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}